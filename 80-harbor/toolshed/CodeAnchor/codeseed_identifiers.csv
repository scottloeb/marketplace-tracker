context,file_name,file_path,identifier_name,identifier_type,line_number,modified_date
"Previous lines: contexts: Documentation and usage contexts associated with identifiers | relationship_map: Network of connections between related identifiers | """" | Line: def __init__(self) -> None:",code_seed.py,code_seed.py,__init__,function,103,2025-03-28
"Previous lines: # Network of relationships between identifier | self.relationship_map: Dict[str, Set[str]] = defaultdict(set | logger.debug(""IdentifierTracker initialized with patterns for multiple languages"" | Line: def extract_identifiers(self, content: str, file_type: str) -> Dict[str, List[Dict[str, Any]]]:",code_seed.py,code_seed.py,extract_identifiers,function,170,2025-03-28
"Previous lines: logger.debug(f""Extracted {sum(len(ids) for ids in identifiers.values())} identifiers | f""of {len(identifiers)} types from {language} content"" | return identifier | Line: def _determine_language(self, file_type: str) -> str:",code_seed.py,code_seed.py,_determine_language,function,235,2025-03-28
"Previous lines: return lan | # Default to a basic set of pattern | return 'python'  # Default to Python pattern | Line: def _extract_context(self, content: str, match: re.Match) -> str:",code_seed.py,code_seed.py,_extract_context,function,275,2025-03-28
"Previous lines: if inline_comment | context.append(""Comment: "" + inline_comment | return "" | "".join(context | Line: def _update_relationships(self, content: str, id_name: str, match: re.Match) -> None:",code_seed.py,code_seed.py,_update_relationships,function,333,2025-03-28
"Previous lines: # Add bidirectional relationshi | self.relationship_map[id_name].add(other_id | self.relationship_map[other_id].add(id_name | Line: def get_identifier_data(self) -> List[Dict[str, Any]]:",code_seed.py,code_seed.py,get_identifier_data,function,373,2025-03-28
"Previous lines: cognitive_markers: Patterns identifying special documentation element | that reveal developer thought processes | """" | Line: def __init__(self) -> None:",code_seed.py,code_seed.py,__init__,function,419,2025-03-28
"Previous lines: 'question': r'\?{2,}',  # Multiple question marks often indicate uncertaint | 'emphasis': r'!{2,}',  # Multiple exclamation points indicate emphasi | logger.debug(""DocumentationExtractor initialized with patterns for multiple languages"" | Line: def extract_documentation(self, content: str, file_type: str) -> Dict[str, List[Dict[str, Any]]]:",code_seed.py,code_seed.py,extract_documentation,function,472,2025-03-28
"Previous lines: logger.debug(f""Extracted {sum(len(docs) for docs in documentation.values())} documentation | f""elements of {len(documentation)} types from {language} content"" | return documentatio | Line: def _determine_language(self, file_type: str) -> str:",code_seed.py,code_seed.py,_determine_language,function,532,2025-03-28
"Previous lines: return lan | # Default to a basic set of pattern | return 'python'  # Default to Python pattern | Line: def _clean_doc_content(self, content: str) -> str:",code_seed.py,code_seed.py,_clean_doc_content,function,572,2025-03-28
"Previous lines: content = re.sub(r'\s+', ' ', content | content = content.strip( | return conten | Line: def _extract_cognitive_markers(self, content: str) -> Dict[str, List[str]]:",code_seed.py,code_seed.py,_extract_cognitive_markers,function,606,2025-03-28
"Previous lines: marker_content = match.group(1) if match.groups() else match.group(0 | markers[marker_type].append(marker_content.strip() | return dict(markers | Line: def get_documentation_data(self, documentation: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:",code_seed.py,code_seed.py,get_documentation_data,function,631,2025-03-28
"Previous lines: pattern_frequencies: Occurrence counts for recognized patterns | signature_elements: Distinctive elements that define code ""signatures"" | """" | Line: def __init__(self) -> None:",code_seed.py,code_seed.py,__init__,function,684,2025-03-28
"Previous lines: 'block_comments': r'/\*.*?\*/' | } | logger.debug(""PatternRecognizer initialized with pattern templates"" | Line: def recognize_patterns(self, content: str, file_type: str) -> Dict[str, int]:",code_seed.py,code_seed.py,recognize_patterns,function,758,2025-03-28
"Previous lines: logger.debug(f""Recognized {sum(pattern_counts.values())} pattern instances | f""across {len(pattern_counts)} pattern types"" | return dict(pattern_counts | Line: def identify_signatures(self, content: str) -> Dict[str, Dict[str, int]]:",code_seed.py,code_seed.py,identify_signatures,function,788,2025-03-28
"Previous lines: if category_counts | signatures[category] = category_count | return signature | Line: def get_dominant_patterns(self) -> List[Dict[str, Any]]:",code_seed.py,code_seed.py,get_dominant_patterns,function,820,2025-03-28
"Previous lines: 'category': self._get_pattern_category(pattern) | } | return pattern_dat | Line: def _get_pattern_category(self, pattern: str) -> str:",code_seed.py,code_seed.py,_get_pattern_category,function,843,2025-03-28
"Previous lines: doc_extractor: System for extracting and analyzing documentation | pattern_recognizer: System for identifying code patterns | """" | Line: def __init__(self) -> None:",code_seed.py,code_seed.py,__init__,function,898,2025-03-28
"Previous lines: self.doc_extractor = DocumentationExtractor( | self.pattern_recognizer = PatternRecognizer( | logger.debug(""FileAnalyzer initialized with specialized analyzers"" | Line: def analyze_file(self, file_path: str) -> Dict[str, Any]:",code_seed.py,code_seed.py,analyze_file,function,913,2025-03-28
"Previous lines: 'path': str(path_obj) | 'name': path_obj.name | 'error': str(e | Line: def _guess_mime_type(self, file_path: str) -> str:",code_seed.py,code_seed.py,_guess_mime_type,function,1020,2025-03-28
"Previous lines: return 'application/octet-stream | # Defaul | return 'application/octet-stream | Line: def _is_text_file(self, mime_type: str) -> bool:",code_seed.py,code_seed.py,_is_text_file,function,1070,2025-03-28
"Previous lines: 'application/json' | 'application/xml' | 'application/x-yaml' | Line: def _hash_content(self, content: str) -> str:",code_seed.py,code_seed.py,_hash_content,function,1092,2025-03-28
"Previous lines: SHA-256 hash of content | """" | return hashlib.sha256(content.encode('utf-8')).hexdigest( | Line: def _extract_file_markers(self, content: str) -> Dict[str, List[str]]:",code_seed.py,code_seed.py,_extract_file_markers,function,1108,2025-03-28
"Previous lines: marker_text = match.group(1) if match.groups() else match.group(0 | markers[marker_type].append(marker_text.strip() | return dict(markers | Line: def _flatten_identifiers(self, identifiers: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:",code_seed.py,code_seed.py,_flatten_identifiers,function,1145,2025-03-28
"Previous lines: 'context': identifier.get('context', '' | flattened.append(record | return flattene | Line: def _flatten_documentation(self, documentation: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:",code_seed.py,code_seed.py,_flatten_documentation,function,1175,2025-03-28
"Previous lines: file_analyzer: System for analyzing individual files | relationship_map: Network of relationships between files | """" | Line: def __init__(self) -> None:",code_seed.py,code_seed.py,__init__,function,1219,2025-03-28
"Previous lines: # Track inaccessible directorie | self.inaccessible_dirs = [ | logger.debug(""DirectoryAnalyzer initialized with file analyzer"" | Line: def scan_directory(self, directory: str, exclude_patterns: List[str] = None) -> Dict[str, Any]:",code_seed.py,code_seed.py,scan_directory,function,1238,2025-03-28
"Previous lines: logger.info(f""Completed directory scan of {directory}"" | logger.info(f""Processed {dir_info['file_count']} files in {dir_info['elapsed_seconds']:.2f} seconds"" | return dir_inf | Line: def _mime_to_language(self, mime_type: str) -> str:",code_seed.py,code_seed.py,_mime_to_language,function,1353,2025-03-28
"Previous lines: 'application/x-yaml': 'YAML' | 'text/x-shellscript': 'Shell Script' | return mime_map.get(mime_type, mime_type | Line: def _update_relationship_map(self, file_info: Dict[str, Any], current_dir: str, base_dir: str) -> None:",code_seed.py,code_seed.py,_update_relationship_map,function,1383,2025-03-28
"Previous lines: self.relationship_map[other_info].add(file_path | except Exception as e | logger.debug(f""Error updating relationship map for {file_path}: {e}"" | Line: def _get_relationship_data(self) -> Dict[str, List[str]]:",code_seed.py,code_seed.py,_get_relationship_data,function,1425,2025-03-28
"Previous lines: for file_path, related_files in self.relationship_map.items() | relationships[file_path] = list(related_files | return relationship | Line: def _extract_pattern_summary(self) -> Dict[str, Any]:",code_seed.py,code_seed.py,_extract_pattern_summary,function,1444,2025-03-28
"Previous lines: Attributes | output_dir: Directory for output files | """" | Line: def __init__(self, output_dir: str = None) -> None:",code_seed.py,code_seed.py,__init__,function,1479,2025-03-28
"Previous lines: # Ensure output directory exist | os.makedirs(self.output_dir, exist_ok=True | logger.debug(f""OutputManager initialized with output directory: {self.output_dir}"" | Line: def write_csv(self, data: List[Dict[str, Any]], filename: str, max_field_length: int = 32000) -> str:",code_seed.py,code_seed.py,write_csv,function,1498,2025-03-28
"Previous lines: writer.writerow(processed_record | logger.info(f""Wrote {len(data)} records to {output_path}"" | return output_pat | Line: def generate_file_csv(self, directory_data: Dict[str, Any], filename: str = ""codeseed_files.csv"") -> str:",code_seed.py,code_seed.py,generate_file_csv,function,1563,2025-03-28
"Previous lines: record[f'marker_{marker}_sample'] = instances[0 | file_records.append(record | return self.write_csv(file_records, filename | Line: def generate_identifier_csv(self, directory_data: Dict[str, Any],",code_seed.py,code_seed.py,generate_identifier_csv,function,1616,2025-03-28
"Previous lines: 'modified_date': file_info.get('modified_date', '') | identifier_records.append(record | return self.write_csv(identifier_records, filename | Line: def generate_documentation_csv(self, directory_data: Dict[str, Any],",code_seed.py,code_seed.py,generate_documentation_csv,function,1656,2025-03-28
"Previous lines: directory_analyzer: System for analyzing directory structures | output_manager: System for generating output files | """" | Line: def __init__(self, output_dir: str = None) -> None:",code_seed.py,code_seed.py,__init__,function,1714,2025-03-28
"Previous lines: self.directory_analyzer = DirectoryAnalyzer( | self.output_manager = OutputManager(output_dir | logger.debug(""CodeSeed initialized with component systems"" | Line: def analyze_directory(self, directory: str,",code_seed.py,code_seed.py,analyze_directory,function,1731,2025-03-28
"Previous lines: output_files['documentation_csv'] = documentation_cs | logger.info(f""Analysis complete. Output files generated in: {self.output_manager.output_dir}"" | return output_file | Line: def analyze_file(self, file_path: str) -> Dict[str, Any]:",code_seed.py,code_seed.py,analyze_file,function,1776,2025-03-28
Previous lines: # ===================================================================== | # Command-Line Interfac | # ====================================================================== | Line: def main():,code_seed.py,code_seed.py,main,function,1796,2025-03-28
Previous lines: # ===================================================================== | # SeedCore: Fundamental Components of the Fores | # ====================================================================== | Line: class IdentifierTracker:,code_seed.py,code_seed.py,IdentifierTracker,class,87,2025-03-28
"Previous lines: 'relationship_count': len(self.relationship_map.get(identifier['name'], set()) | identifier_data.append(record | return identifier_dat | Line: class DocumentationExtractor:",code_seed.py,code_seed.py,DocumentationExtractor,class,403,2025-03-28
Previous lines: record[f'marker_{marker_type}'] = '; '.join(instances | doc_data.append(record | return doc_dat | Line: class PatternRecognizer:,code_seed.py,code_seed.py,PatternRecognizer,class,669,2025-03-28
Previous lines: # ===================================================================== | # SeedMapper: Core Analysis Engin | # ====================================================================== | Line: class FileAnalyzer:,code_seed.py,code_seed.py,FileAnalyzer,class,884,2025-03-28
"Previous lines: 'has_markers': bool(doc.get('markers', {}) | flattened.append(record | return flattene | Line: class DirectoryAnalyzer:",code_seed.py,code_seed.py,DirectoryAnalyzer,class,1205,2025-03-28
"Previous lines: category = pattern.get('category', 'other' | patterns_by_category[category].append(pattern | return dict(patterns_by_category | Line: class OutputManager:",code_seed.py,code_seed.py,OutputManager,class,1467,2025-03-28
Previous lines: # ===================================================================== | # SeedCore: Main AP | # ====================================================================== | Line: class CodeSeed:,code_seed.py,code_seed.py,CodeSeed,class,1701,2025-03-28
"Previous lines: syntax or dependencies, CodeSeed maps the thoughtflows embedded in code throug | a rich metadata extraction process. | This tool embodies a forest-first approach to code analysis | Line: - Trees = Individual files with unique growth patterns",code_seed.py,code_seed.py,Trees,variable,11,2025-03-28
Previous lines: a rich metadata extraction process. | This tool embodies a forest-first approach to code analysis | - Trees = Individual files with unique growth pattern | Line: - Roots = Core dependencies and foundations,code_seed.py,code_seed.py,Roots,variable,12,2025-03-28
Previous lines: This tool embodies a forest-first approach to code analysis | - Trees = Individual files with unique growth pattern | - Roots = Core dependencies and foundation | Line: - Canopy = Interface layers that interact with users,code_seed.py,code_seed.py,Canopy,variable,13,2025-03-28
Previous lines: - Trees = Individual files with unique growth pattern | - Roots = Core dependencies and foundation | - Canopy = Interface layers that interact with user | Line: - Seeds = Reusable patterns and concepts that can spread,code_seed.py,code_seed.py,Seeds,variable,14,2025-03-28
Previous lines: - Roots = Core dependencies and foundation | - Canopy = Interface layers that interact with user | - Seeds = Reusable patterns and concepts that can sprea | Line: - Forest Floor = Shared resources and utilities,code_seed.py,code_seed.py,Floor,variable,15,2025-03-28
"Previous lines: """""" | # Configure logging to track analysis proces | logging.basicConfig | Line: level=logging.INFO,",code_seed.py,code_seed.py,level,variable,76,2025-03-28
"Previous lines: # Configure logging to track analysis proces | logging.basicConfig | level=logging.INFO | Line: format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',",code_seed.py,code_seed.py,format,variable,77,2025-03-28
Previous lines: logging.basicConfig | level=logging.INFO | format='%(asctime)s - %(name)s - %(levelname)s - %(message)s' | Line: datefmt='%Y-%m-%d %H:%M:%S',code_seed.py,code_seed.py,datefmt,variable,78,2025-03-28
Previous lines: level=logging.INFO | format='%(asctime)s - %(name)s - %(levelname)s - %(message)s' | datefmt='%Y-%m-%d %H:%M:%S | Line: logger = logging.getLogger('codeseed'),code_seed.py,code_seed.py,logger,variable,80,2025-03-28
"Previous lines: used throughout the analysis process | """" | # Language-specific identifier pattern | Line: self.patterns = {",code_seed.py,code_seed.py,patterns,variable,112,2025-03-28
"Previous lines: } | # HTML identifier pattern | 'html': | Line: 'id': r'id=[\'""]([^\'""]*)[\'""]',",code_seed.py,code_seed.py,id,variable,138,2025-03-28
"Previous lines: # HTML identifier pattern | 'html': | 'id': r'id=[\'""]([^\'""]*)[\'""]' | Line: 'class': r'class=[\'""]([^\'""]*)[\'""]',",code_seed.py,code_seed.py,class,variable,139,2025-03-28
"Previous lines: Returns | Dictionary mapping identifier types to lists of found identifiers | """" | Line: language = self._determine_language(file_type)",code_seed.py,code_seed.py,language,variable,185,2025-03-28
"Previous lines: Dictionary mapping identifier types to lists of found identifiers | """" | language = self._determine_language(file_type | Line: patterns = self.patterns.get(language, {})",code_seed.py,code_seed.py,patterns,variable,186,2025-03-28
"Previous lines: identifiers: Dict[str, List[Dict[str, Any]]] = defaultdict(list | # Apply each pattern for the detected languag | for id_type, pattern in patterns.items() | Line: matches = re.finditer(pattern, content, re.MULTILINE)",code_seed.py,code_seed.py,matches,variable,193,2025-03-28
Previous lines: # Extract and store each match with its contex | for match in matches | # Handle special cases like parameters that need parsin | Line: if id_type == 'parameter':,code_seed.py,code_seed.py,id_type,variable,198,2025-03-28
Previous lines: for match in matches | # Handle special cases like parameters that need parsin | if id_type == 'parameter' | Line: param_str = match.group(1).strip(),code_seed.py,code_seed.py,param_str,variable,199,2025-03-28
"Previous lines: param_str = match.group(1).strip( | if param_str | # Split and process individual parameter | Line: params = [p.strip() for p in param_str.split(',')]",code_seed.py,code_seed.py,params,variable,202,2025-03-28
"Previous lines: params = [p.strip() for p in param_str.split(',') | for param in params | # Handle default values, type hints, etc | Line: param_name = param.split('=')[0].split(':')[0].strip()",code_seed.py,code_seed.py,param_name,variable,205,2025-03-28
Previous lines: param_name = param.split('=')[0].split(':')[0].strip( | if param_name and param_name != 'self' | # Create an identifier recor | Line: identifier = {,code_seed.py,code_seed.py,identifier,variable,208,2025-03-28
Previous lines: identifiers['parameter'].append(identifier | else | # Standard identifier extractio | Line: id_name = match.group(1),code_seed.py,code_seed.py,id_name,variable,218,2025-03-28
Previous lines: else | # Standard identifier extractio | id_name = match.group(1 | Line: identifier = {,code_seed.py,code_seed.py,identifier,variable,219,2025-03-28
"Previous lines: Language key for pattern lookup | """" | # Map common file types/extensions to language pattern set | Line: type_to_language = {",code_seed.py,code_seed.py,type_to_language,variable,249,2025-03-28
"Previous lines: Context string surrounding the identifier | """" | # Find the start and end of the line containing the matc | Line: line_start = content.rfind('\n', 0, match.start()) + 1",code_seed.py,code_seed.py,line_start,variable,291,2025-03-28
"Previous lines: """" | # Find the start and end of the line containing the matc | line_start = content.rfind('\n', 0, match.start()) + | Line: line_end = content.find('\n', match.end())",code_seed.py,code_seed.py,line_end,variable,292,2025-03-28
"Previous lines: # Find the start and end of the line containing the matc | line_start = content.rfind('\n', 0, match.start()) + | line_end = content.find('\n', match.end() | Line: if line_end == -1:",code_seed.py,code_seed.py,line_end,variable,293,2025-03-28
"Previous lines: line_start = content.rfind('\n', 0, match.start()) + | line_end = content.find('\n', match.end() | if line_end == -1 | Line: line_end = len(content)",code_seed.py,code_seed.py,line_end,variable,294,2025-03-28
Previous lines: if line_end == -1 | line_end = len(content | # Get the content of the lin | Line: line = content[line_start:line_end].strip(),code_seed.py,code_seed.py,line,variable,297,2025-03-28
Previous lines: # Get the content of the lin | line = content[line_start:line_end].strip( | # Check for comments on the same lin | Line: comment_start = line.find('#') | Comment: #'),code_seed.py,code_seed.py,comment_start,variable,300,2025-03-28
Previous lines: # Check for comments on the same lin | comment_start = line.find('#' | if comment_start != -1 | Line: inline_comment = line[comment_start:].strip(),code_seed.py,code_seed.py,inline_comment,variable,302,2025-03-28
"Previous lines: if comment_start != -1 | inline_comment = line[comment_start:].strip( | else | Line: inline_comment = """"",code_seed.py,code_seed.py,inline_comment,variable,304,2025-03-28
"Previous lines: else | inline_comment = "" | # Look for docstring or comment abov | Line: prev_lines = []",code_seed.py,code_seed.py,prev_lines,variable,307,2025-03-28
"Previous lines: inline_comment = "" | # Look for docstring or comment abov | prev_lines = [ | Line: current_pos = line_start - 2  # Start before the newline | Comment: # Start before the newline",code_seed.py,code_seed.py,current_pos,variable,308,2025-03-28
Previous lines: prev_lines = [ | current_pos = line_start - 2  # Start before the newlin | # Collect up to 3 non-empty lines abov | Line: line_count = 0,code_seed.py,code_seed.py,line_count,variable,311,2025-03-28
Previous lines: # Collect up to 3 non-empty lines abov | line_count = | while current_pos >= 0 and line_count < 3 | Line: prev_line_end = current_pos,code_seed.py,code_seed.py,prev_line_end,variable,313,2025-03-28
"Previous lines: line_count = | while current_pos >= 0 and line_count < 3 | prev_line_end = current_po | Line: prev_line_start = content.rfind('\n', 0, prev_line_end) + 1",code_seed.py,code_seed.py,prev_line_start,variable,314,2025-03-28
"Previous lines: while current_pos >= 0 and line_count < 3 | prev_line_end = current_po | prev_line_start = content.rfind('\n', 0, prev_line_end) + | Line: prev_line = content[prev_line_start:prev_line_end].strip()",code_seed.py,code_seed.py,prev_line,variable,316,2025-03-28
"Previous lines: if prev_line | prev_lines.insert(0, prev_line | line_count += | Line: current_pos = prev_line_start - 2  # Move to the next line up | Comment: # Move to the next line up",code_seed.py,code_seed.py,current_pos,variable,321,2025-03-28
Previous lines: line_count += | current_pos = prev_line_start - 2  # Move to the next line u | # Combine collected contex | Line: context = [],code_seed.py,code_seed.py,context,variable,324,2025-03-28
"Previous lines: retur | # Find related identifiers in the same scop | # (simplified: look in the same function/class or nearby | Line: scope_start = content.rfind('\n\n', 0, match.start())",code_seed.py,code_seed.py,scope_start,variable,352,2025-03-28
"Previous lines: # Find related identifiers in the same scop | # (simplified: look in the same function/class or nearby | scope_start = content.rfind('\n\n', 0, match.start() | Line: if scope_start == -1:",code_seed.py,code_seed.py,scope_start,variable,353,2025-03-28
"Previous lines: # (simplified: look in the same function/class or nearby | scope_start = content.rfind('\n\n', 0, match.start() | if scope_start == -1 | Line: scope_start = 0",code_seed.py,code_seed.py,scope_start,variable,354,2025-03-28
"Previous lines: scope_start = content.rfind('\n\n', 0, match.start() | if scope_start == -1 | scope_start = | Line: scope_end = content.find('\n\n', match.end())",code_seed.py,code_seed.py,scope_end,variable,356,2025-03-28
"Previous lines: if scope_start == -1 | scope_start = | scope_end = content.find('\n\n', match.end() | Line: if scope_end == -1:",code_seed.py,code_seed.py,scope_end,variable,357,2025-03-28
"Previous lines: scope_start = | scope_end = content.find('\n\n', match.end() | if scope_end == -1 | Line: scope_end = len(content)",code_seed.py,code_seed.py,scope_end,variable,358,2025-03-28
"Previous lines: scope_end = content.find('\n\n', match.end() | if scope_end == -1 | scope_end = len(content | Line: scope = content[scope_start:scope_end]",code_seed.py,code_seed.py,scope,variable,360,2025-03-28
"Previous lines: # Find other identifiers in this scop | for pattern_type, patterns in self.patterns.items() | for _, pattern in patterns.items() | Line: other_matches = re.finditer(pattern, scope, re.MULTILINE)",code_seed.py,code_seed.py,other_matches,variable,365,2025-03-28
"Previous lines: for _, pattern in patterns.items() | other_matches = re.finditer(pattern, scope, re.MULTILINE | for other_match in other_matches | Line: other_id = other_match.group(1)",code_seed.py,code_seed.py,other_id,variable,367,2025-03-28
"Previous lines: Returns | List of identifier records with full metadata | """" | Line: identifier_data = []",code_seed.py,code_seed.py,identifier_data,variable,384,2025-03-28
"Previous lines: for id_type, identifiers in self.contexts.items() | # Process each identifier instanc | for identifier in identifiers | Line: record = {",code_seed.py,code_seed.py,record,variable,390,2025-03-28
"Previous lines: of documentation, from formal docstrings to quick notes and TODOs | """" | # Patterns for documentation extractio | Line: self.doc_patterns = {",code_seed.py,code_seed.py,doc_patterns,variable,427,2025-03-28
"Previous lines: # HTML documentation pattern | 'html': | 'comment': r'<!--(.*?)-->' | Line: 'meta_description': r'<meta\s+name=[\'""]description[\'""]\s+content=[\'""]([^\'""]*)[\'""]'}",code_seed.py,code_seed.py,name,variable,445,2025-03-28
"Previous lines: # HTML documentation pattern | 'html': | 'comment': r'<!--(.*?)-->' | Line: 'meta_description': r'<meta\s+name=[\'""]description[\'""]\s+content=[\'""]([^\'""]*)[\'""]'}",code_seed.py,code_seed.py,content,variable,445,2025-03-28
Previous lines: 'markdown': | 'code_comment': r'<!--(.*?)-->' | # Patterns for identifying cognitive marker | Line: self.cognitive_markers = {,code_seed.py,code_seed.py,cognitive_markers,variable,458,2025-03-28
"Previous lines: Returns | Dictionary mapping documentation types to lists of extracted elements | """" | Line: language = self._determine_language(file_type)",code_seed.py,code_seed.py,language,variable,487,2025-03-28
"Previous lines: Dictionary mapping documentation types to lists of extracted elements | """" | language = self._determine_language(file_type | Line: patterns = self.doc_patterns.get(language, {})",code_seed.py,code_seed.py,patterns,variable,488,2025-03-28
"Previous lines: documentation: Dict[str, List[Dict[str, Any]]] = defaultdict(list | # Extract standard documentation based on languag | for doc_type, pattern in patterns.items() | Line: matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL)",code_seed.py,code_seed.py,matches,variable,495,2025-03-28
"Previous lines: for doc_type, pattern in patterns.items() | matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL | for match in matches | Line: doc_content = match.group(1) if match.groups() else """"",code_seed.py,code_seed.py,doc_content,variable,498,2025-03-28
"Previous lines: matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL | for match in matches | doc_content = match.group(1) if match.groups() else "" | Line: doc_content = self._clean_doc_content(doc_content)",code_seed.py,code_seed.py,doc_content,variable,499,2025-03-28
Previous lines: if not doc_content.strip() | continu | # Create documentation recor | Line: doc_record = {,code_seed.py,code_seed.py,doc_record,variable,506,2025-03-28
"Previous lines: documentation[doc_type].append(doc_record | # Extract cognitive markers across all conten | for marker_type, pattern in self.cognitive_markers.items() | Line: matches = re.finditer(pattern, content, re.MULTILINE | re.IGNORECASE)",code_seed.py,code_seed.py,matches,variable,517,2025-03-28
"Previous lines: for marker_type, pattern in self.cognitive_markers.items() | matches = re.finditer(pattern, content, re.MULTILINE | re.IGNORECASE | for match in matches | Line: marker_content = match.group(1) if match.groups() else match.group(0)",code_seed.py,code_seed.py,marker_content,variable,520,2025-03-28
"Previous lines: matches = re.finditer(pattern, content, re.MULTILINE | re.IGNORECASE | for match in matches | marker_content = match.group(1) if match.groups() else match.group(0 | Line: marker_record = {",code_seed.py,code_seed.py,marker_record,variable,521,2025-03-28
"Previous lines: Language key for pattern lookup | """" | # Map common file types/extensions to language pattern set | Line: type_to_language = {",code_seed.py,code_seed.py,type_to_language,variable,546,2025-03-28
"Previous lines: if not content | return "" | # Remove common indentatio | Line: lines = content.split('\n')",code_seed.py,code_seed.py,lines,variable,589,2025-03-28
Previous lines: lines = content.split('\n' | if len(lines) > 1 | # Find minimum indentation (excluding empty lines | Line: indents = [len(line) - len(line.lstrip()),code_seed.py,code_seed.py,indents,variable,592,2025-03-28
Previous lines: indents = [len(line) - len(line.lstrip()) | for line in lines[1:] if line.strip() | if indents | Line: min_indent = min(indents),code_seed.py,code_seed.py,min_indent,variable,595,2025-03-28
Previous lines: # Remove the common indentation from all lines after the firs | lines[1:] = [line[min_indent:] if line.strip() else line for line in lines[1:] | # Rejoin and normalize whitespac | Line: content = '\n'.join(lines),code_seed.py,code_seed.py,content,variable,600,2025-03-28
"Previous lines: lines[1:] = [line[min_indent:] if line.strip() else line for line in lines[1:] | # Rejoin and normalize whitespac | content = '\n'.join(lines | Line: content = re.sub(r'\s+', ' ', content)",code_seed.py,code_seed.py,content,variable,601,2025-03-28
"Previous lines: # Rejoin and normalize whitespac | content = '\n'.join(lines | content = re.sub(r'\s+', ' ', content | Line: content = content.strip()",code_seed.py,code_seed.py,content,variable,602,2025-03-28
"Previous lines: markers: Dict[str, List[str]] = defaultdict(list | # Check for each marker typ | for marker_type, pattern in self.cognitive_markers.items() | Line: matches = re.finditer(pattern, content, re.MULTILINE | re.IGNORECASE)",code_seed.py,code_seed.py,matches,variable,623,2025-03-28
"Previous lines: for marker_type, pattern in self.cognitive_markers.items() | matches = re.finditer(pattern, content, re.MULTILINE | re.IGNORECASE | for match in matches | Line: marker_content = match.group(1) if match.groups() else match.group(0)",code_seed.py,code_seed.py,marker_content,variable,626,2025-03-28
"Previous lines: Returns | List of documentation records with full metadata | """" | Line: doc_data = []",code_seed.py,code_seed.py,doc_data,variable,644,2025-03-28
"Previous lines: for doc_type, elements in documentation.items() | # Process each documentation elemen | for element in elements | Line: record = {",code_seed.py,code_seed.py,record,variable,650,2025-03-28
"Previous lines: coding patterns, architectural approaches, and design signatures | """" | # Common code pattern templates | Line: self.code_patterns = {",code_seed.py,code_seed.py,code_patterns,variable,692,2025-03-28
Previous lines: 'assert_statement': r'assert\s+' | 'mock_setup': r'mock\s*\(' | # Counter for pattern occurrence | Line: self.pattern_frequencies = Counter(),code_seed.py,code_seed.py,pattern_frequencies,variable,730,2025-03-28
Previous lines: # Counter for pattern occurrence | self.pattern_frequencies = Counter( | # Code signature elements to identif | Line: self.signature_elements = {,code_seed.py,code_seed.py,signature_elements,variable,733,2025-03-28
"Previous lines: Returns | Dictionary mapping pattern names to occurrence counts | """" | Line: pattern_counts = Counter()",code_seed.py,code_seed.py,pattern_counts,variable,773,2025-03-28
"Previous lines: pattern_counts = Counter( | # Apply each patter | for pattern_name, pattern in self.code_patterns.items() | Line: matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL)",code_seed.py,code_seed.py,matches,variable,777,2025-03-28
"Previous lines: # Apply each patter | for pattern_name, pattern in self.code_patterns.items() | matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL | Line: count = sum(1 for _ in matches)",code_seed.py,code_seed.py,count,variable,778,2025-03-28
"Previous lines: Returns | Nested dictionary mapping signature categories to element frequencies | """" | Line: signatures = {}",code_seed.py,code_seed.py,signatures,variable,802,2025-03-28
"Previous lines: signatures = { | # Check each signature element categor | for category, elements in self.signature_elements.items() | Line: category_counts = {}",code_seed.py,code_seed.py,category_counts,variable,806,2025-03-28
"Previous lines: for category, elements in self.signature_elements.items() | category_counts = { | for element_name, pattern in elements.items() | Line: matches = re.finditer(pattern, content, re.MULTILINE)",code_seed.py,code_seed.py,matches,variable,809,2025-03-28
"Previous lines: category_counts = { | for element_name, pattern in elements.items() | matches = re.finditer(pattern, content, re.MULTILINE | Line: count = sum(1 for _ in matches)",code_seed.py,code_seed.py,count,variable,810,2025-03-28
"Previous lines: Returns | List of pattern records with frequency data | """" | Line: pattern_data = []",code_seed.py,code_seed.py,pattern_data,variable,831,2025-03-28
"Previous lines: approach to understanding code structure and patterns | """" | # Initialize analysis component | Line: self.identifier_tracker = IdentifierTracker()",code_seed.py,code_seed.py,identifier_tracker,variable,907,2025-03-28
"Previous lines: """" | # Initialize analysis component | self.identifier_tracker = IdentifierTracker( | Line: self.doc_extractor = DocumentationExtractor()",code_seed.py,code_seed.py,doc_extractor,variable,908,2025-03-28
Previous lines: # Initialize analysis component | self.identifier_tracker = IdentifierTracker( | self.doc_extractor = DocumentationExtractor( | Line: self.pattern_recognizer = PatternRecognizer(),code_seed.py,code_seed.py,pattern_recognizer,variable,909,2025-03-28
"Previous lines: Returns | Dictionary containing comprehensive file metadata | """" | Line: path_obj = Path(file_path)",code_seed.py,code_seed.py,path_obj,variable,927,2025-03-28
Previous lines: path_obj = Path(file_path | try | # Basic file metadat | Line: file_stats = path_obj.stat(),code_seed.py,code_seed.py,file_stats,variable,931,2025-03-28
Previous lines: try | # Basic file metadat | file_stats = path_obj.stat( | Line: file_info = {,code_seed.py,code_seed.py,file_info,variable,932,2025-03-28
Previous lines: 'modified_date': datetime.datetime.fromtimestamp(file_stats.st_mtime).strftime('%Y-%m-%d') | 'created_date': datetime.datetime.fromtimestamp(file_stats.st_ctime).strftime('%Y-%m-%d') | # Determine if this is a text file we should analyze in detai | Line: mime_type = self._guess_mime_type(file_path),code_seed.py,code_seed.py,mime_type,variable,946,2025-03-28
"Previous lines: if self._is_text_file(mime_type) and file_stats.st_size < 1000000:  # Limit to 1M | try | # Read file conten | Line: with open(file_path, 'r', encoding='utf-8', errors='replace') as f:",code_seed.py,code_seed.py,encoding,variable,953,2025-03-28
"Previous lines: if self._is_text_file(mime_type) and file_stats.st_size < 1000000:  # Limit to 1M | try | # Read file conten | Line: with open(file_path, 'r', encoding='utf-8', errors='replace') as f:",code_seed.py,code_seed.py,errors,variable,953,2025-03-28
"Previous lines: try | # Read file conten | with open(file_path, 'r', encoding='utf-8', errors='replace') as f | Line: content = f.read()",code_seed.py,code_seed.py,content,variable,954,2025-03-28
"Previous lines: with open(file_path, 'r', encoding='utf-8', errors='replace') as f | content = f.read( | # Basic content statistic | Line: lines = content.split('\n')",code_seed.py,code_seed.py,lines,variable,957,2025-03-28
"Previous lines: 'content_hash': self._hash_content(content) | } | # Extract identifier | Line: identifiers = self.identifier_tracker.extract_identifiers(content, mime_type)",code_seed.py,code_seed.py,identifiers,variable,966,2025-03-28
"Previous lines: file_info['identifier_counts'] = {k: len(v) for k, v in identifiers.items() | file_info['total_identifiers'] = sum(len(v) for v in identifiers.values() | # Extract documentatio | Line: documentation = self.doc_extractor.extract_documentation(content, mime_type)",code_seed.py,code_seed.py,documentation,variable,971,2025-03-28
"Previous lines: file_info['documentation_counts'] = {k: len(v) for k, v in documentation.items() | file_info['total_documentation'] = sum(len(v) for v in documentation.values() | # Recognize pattern | Line: patterns = self.pattern_recognizer.recognize_patterns(content, mime_type)",code_seed.py,code_seed.py,patterns,variable,976,2025-03-28
Previous lines: file_info['pattern_counts'] = pattern | file_info['total_patterns'] = sum(patterns.values() | # Identify code signature | Line: signatures = self.pattern_recognizer.identify_signatures(content),code_seed.py,code_seed.py,signatures,variable,981,2025-03-28
Previous lines: else | file_info['documentation_density'] = | # Extract cognitive marker | Line: cognitive_markers = self._extract_file_markers(content),code_seed.py,code_seed.py,cognitive_markers,variable,991,2025-03-28
"Previous lines: Returns | MIME type string | """" | Line: extension = Path(file_path).suffix.lower()",code_seed.py,code_seed.py,extension,variable,1034,2025-03-28
"Previous lines: """" | extension = Path(file_path).suffix.lower( | # Common extensions to MIME type | Line: extension_map = {",code_seed.py,code_seed.py,extension_map,variable,1037,2025-03-28
"Previous lines: Returns | Dictionary mapping marker types to instances | """" | Line: markers = defaultdict(list)",code_seed.py,code_seed.py,markers,variable,1122,2025-03-28
"Previous lines: """" | markers = defaultdict(list | # Common cognitive marker | Line: marker_patterns = {",code_seed.py,code_seed.py,marker_patterns,variable,1125,2025-03-28
"Previous lines: 'emoji': r'([🌱🔍🧩🚀🔧🌉🧠🔄🪢🔨])',  # Track emoji usag | # Extract each marker typ | for marker_type, pattern in marker_patterns.items() | Line: matches = re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE)",code_seed.py,code_seed.py,matches,variable,1138,2025-03-28
"Previous lines: for marker_type, pattern in marker_patterns.items() | matches = re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE | for match in matches | Line: marker_text = match.group(1) if match.groups() else match.group(0)",code_seed.py,code_seed.py,marker_text,variable,1140,2025-03-28
"Previous lines: Returns | Flattened list of identifier records | """" | Line: flattened = []",code_seed.py,code_seed.py,flattened,variable,1159,2025-03-28
"Previous lines: for id_type, id_list in identifiers.items() | # Process each identifie | for identifier in id_list | Line: record = {",code_seed.py,code_seed.py,record,variable,1165,2025-03-28
"Previous lines: Returns | Flattened list of documentation records | """" | Line: flattened = []",code_seed.py,code_seed.py,flattened,variable,1188,2025-03-28
"Previous lines: for doc_type, doc_list in documentation.items() | # Process each documentation elemen | for doc in doc_list | Line: record = {",code_seed.py,code_seed.py,record,variable,1194,2025-03-28
"Previous lines: comprehensive approach to mapping the code forest | """" | # Initialize file analyzer componen | Line: self.file_analyzer = FileAnalyzer()",code_seed.py,code_seed.py,file_analyzer,variable,1228,2025-03-28
Previous lines: # Initialize file analyzer componen | self.file_analyzer = FileAnalyzer( | # Map of file relationship | Line: self.relationship_map = defaultdict(set),code_seed.py,code_seed.py,relationship_map,variable,1231,2025-03-28
Previous lines: # Map of file relationship | self.relationship_map = defaultdict(set | # Track inaccessible directorie | Line: self.inaccessible_dirs = [],code_seed.py,code_seed.py,inaccessible_dirs,variable,1234,2025-03-28
"Previous lines: Returns | Dictionary containing comprehensive directory metadata | """" | Line: dir_path = Path(directory)",code_seed.py,code_seed.py,dir_path,variable,1253,2025-03-28
"Previous lines: """" | dir_path = Path(directory | # Compile exclude pattern | Line: exclude_regex = None",code_seed.py,code_seed.py,exclude_regex,variable,1256,2025-03-28
Previous lines: # Compile exclude pattern | exclude_regex = Non | if exclude_patterns | Line: exclude_pattern = '|'.join(f'({pattern})' for pattern in exclude_patterns),code_seed.py,code_seed.py,exclude_pattern,variable,1258,2025-03-28
Previous lines: exclude_regex = Non | if exclude_patterns | exclude_pattern = '|'.join(f'({pattern})' for pattern in exclude_patterns | Line: exclude_regex = re.compile(exclude_pattern),code_seed.py,code_seed.py,exclude_regex,variable,1259,2025-03-28
Previous lines: exclude_pattern = '|'.join(f'({pattern})' for pattern in exclude_patterns | exclude_regex = re.compile(exclude_pattern | # Initialize directory metadat | Line: dir_info = {,code_seed.py,code_seed.py,dir_info,variable,1262,2025-03-28
"Previous lines: # Walk the directory tre | for root, dirs, files in os.walk(directory) | # Check if this directory should be exclude | Line: rel_root = os.path.relpath(root, directory)",code_seed.py,code_seed.py,rel_root,variable,1277,2025-03-28
"Previous lines: # Process each fil | for filename in files | # Check if file should be exclude | Line: file_path = os.path.join(root, filename)",code_seed.py,code_seed.py,file_path,variable,1296,2025-03-28
"Previous lines: for filename in files | # Check if file should be exclude | file_path = os.path.join(root, filename | Line: rel_path = os.path.relpath(file_path, directory)",code_seed.py,code_seed.py,rel_path,variable,1297,2025-03-28
Previous lines: continu | # Analyze fil | try | Line: file_info = self.file_analyzer.analyze_file(file_path),code_seed.py,code_seed.py,file_info,variable,1304,2025-03-28
"Previous lines: dir_info['file_count'] += | dir_info['total_size_bytes'] += file_info.get('size_bytes', 0 | # Update extension statistic | Line: extension = file_info.get('extension', '').lower()",code_seed.py,code_seed.py,extension,variable,1311,2025-03-28
"Previous lines: if extension | dir_info['file_extensions'][extension] = dir_info['file_extensions'].get(extension, 0) + | # Update language statistics based on MIME typ | Line: mime_type = file_info.get('mime_type', '')",code_seed.py,code_seed.py,mime_type,variable,1316,2025-03-28
"Previous lines: dir_info['file_extensions'][extension] = dir_info['file_extensions'].get(extension, 0) + | # Update language statistics based on MIME typ | mime_type = file_info.get('mime_type', '' | Line: language = self._mime_to_language(mime_type)",code_seed.py,code_seed.py,language,variable,1317,2025-03-28
Previous lines: dir_info['files'].append(file_info | # Log progress periodicall | if dir_info['file_count'] % 100 == 0 | Line: elapsed = time.time() - dir_info['start_time'],code_seed.py,code_seed.py,elapsed,variable,1329,2025-03-28
"Previous lines: Returns | Language name | """" | Line: mime_map = {",code_seed.py,code_seed.py,mime_map,variable,1366,2025-03-28
"Previous lines: current_dir: Current directory being processed | base_dir: Base directory of the scan | """" | Line: file_path = file_info.get('path')",code_seed.py,code_seed.py,file_path,variable,1396,2025-03-28
Previous lines: retur | try | # Find related files based on directory structur | Line: parent_dir = os.path.basename(current_dir),code_seed.py,code_seed.py,parent_dir,variable,1402,2025-03-28
"Previous lines: # Find related files based on directory structur | parent_dir = os.path.basename(current_dir | for other_file in os.listdir(current_dir) | Line: other_path = os.path.join(current_dir, other_file)",code_seed.py,code_seed.py,other_path,variable,1404,2025-03-28
"Previous lines: self.relationship_map[file_path].add(other_path | self.relationship_map[other_path].add(file_path | # Find related files based on name pattern | Line: file_name = file_info.get('name', '')",code_seed.py,code_seed.py,file_name,variable,1411,2025-03-28
"Previous lines: self.relationship_map[other_path].add(file_path | # Find related files based on name pattern | file_name = file_info.get('name', '' | Line: base_name = os.path.splitext(file_name)[0]",code_seed.py,code_seed.py,base_name,variable,1412,2025-03-28
"Previous lines: Returns | Dictionary mapping file paths to lists of related files | """" | Line: relationships = {}",code_seed.py,code_seed.py,relationships,variable,1436,2025-03-28
"Previous lines: Dictionary containing pattern summary data | """" | # Get dominant pattern | Line: dominant_patterns = self.file_analyzer.pattern_recognizer.get_dominant_patterns()",code_seed.py,code_seed.py,dominant_patterns,variable,1456,2025-03-28
Previous lines: # Get dominant pattern | dominant_patterns = self.file_analyzer.pattern_recognizer.get_dominant_patterns( | # Group patterns by categor | Line: patterns_by_category = defaultdict(list),code_seed.py,code_seed.py,patterns_by_category,variable,1459,2025-03-28
"Previous lines: # Group patterns by categor | patterns_by_category = defaultdict(list | for pattern in dominant_patterns | Line: category = pattern.get('category', 'other')",code_seed.py,code_seed.py,category,variable,1461,2025-03-28
"Previous lines: Attributes | output_dir: Directory for output files | """" | Line: def __init__(self, output_dir: str = None) -> None:",code_seed.py,code_seed.py,str,variable,1479,2025-03-28
"Previous lines: output_dir: Directory for output files (defaults to current directory) | """" | # Set output director | Line: self.output_dir = output_dir or '.'",code_seed.py,code_seed.py,output_dir,variable,1491,2025-03-28
"Previous lines: # Set output director | self.output_dir = output_dir or '. | # Ensure output directory exist | Line: os.makedirs(self.output_dir, exist_ok=True)",code_seed.py,code_seed.py,exist_ok,variable,1494,2025-03-28
"Previous lines: # Ensure output directory exist | os.makedirs(self.output_dir, exist_ok=True | logger.debug(f""OutputManager initialized with output directory: {self.output_dir}"" | Line: def write_csv(self, data: List[Dict[str, Any]], filename: str, max_field_length: int = 32000) -> str:",code_seed.py,code_seed.py,int,variable,1498,2025-03-28
"Previous lines: logger.warning(""No data to write to CSV"" | return "" | # Prepare output pat | Line: output_path = os.path.join(self.output_dir, filename)",code_seed.py,code_seed.py,output_path,variable,1520,2025-03-28
"Previous lines: # Prepare output pat | output_path = os.path.join(self.output_dir, filename | # Collect all field name | Line: all_fields = set()",code_seed.py,code_seed.py,all_fields,variable,1523,2025-03-28
Previous lines: for record in data | all_fields.update(record.keys() | # Sort fields for consistent outpu | Line: fields = sorted(list(all_fields)),code_seed.py,code_seed.py,fields,variable,1528,2025-03-28
"Previous lines: # Sort fields for consistent outpu | fields = sorted(list(all_fields) | # Write to CS | Line: with open(output_path, 'w', newline='', encoding='utf-8') as f:",code_seed.py,code_seed.py,newline,variable,1531,2025-03-28
"Previous lines: # Sort fields for consistent outpu | fields = sorted(list(all_fields) | # Write to CS | Line: with open(output_path, 'w', newline='', encoding='utf-8') as f:",code_seed.py,code_seed.py,encoding,variable,1531,2025-03-28
"Previous lines: fields = sorted(list(all_fields) | # Write to CS | with open(output_path, 'w', newline='', encoding='utf-8') as f | Line: writer = csv.DictWriter(f, fieldnames=fields, extrasaction='ignore')",code_seed.py,code_seed.py,writer,variable,1532,2025-03-28
"Previous lines: fields = sorted(list(all_fields) | # Write to CS | with open(output_path, 'w', newline='', encoding='utf-8') as f | Line: writer = csv.DictWriter(f, fieldnames=fields, extrasaction='ignore')",code_seed.py,code_seed.py,fieldnames,variable,1532,2025-03-28
"Previous lines: fields = sorted(list(all_fields) | # Write to CS | with open(output_path, 'w', newline='', encoding='utf-8') as f | Line: writer = csv.DictWriter(f, fieldnames=fields, extrasaction='ignore')",code_seed.py,code_seed.py,extrasaction,variable,1532,2025-03-28
"Previous lines: # Write each record, truncating long field | for record in data | # Process record to handle special values and truncat | Line: processed_record = {}",code_seed.py,code_seed.py,processed_record,variable,1538,2025-03-28
"Previous lines: continu | if isinstance(value, (dict, list)) | # Convert complex types to JSON string | Line: value = json.dumps(value)",code_seed.py,code_seed.py,value,variable,1545,2025-03-28
"Previous lines: # Convert complex types to JSON string | value = json.dumps(value | elif value is None | Line: value = """"",code_seed.py,code_seed.py,value,variable,1547,2025-03-28
"Previous lines: value = "" | else | # Convert to strin | Line: value = str(value)",code_seed.py,code_seed.py,value,variable,1550,2025-03-28
"Previous lines: value = str(value | # Truncate if too lon | if len(value) > max_field_length | Line: value = value[:max_field_length - 3] + ""...""",code_seed.py,code_seed.py,value,variable,1554,2025-03-28
"Previous lines: writer.writerow(processed_record | logger.info(f""Wrote {len(data)} records to {output_path}"" | return output_pat | Line: def generate_file_csv(self, directory_data: Dict[str, Any], filename: str = ""codeseed_files.csv"") -> str:",code_seed.py,code_seed.py,str,variable,1563,2025-03-28
"Previous lines: Returns | Path to the created CSV file | """" | Line: files = directory_data.get('files', [])",code_seed.py,code_seed.py,files,variable,1578,2025-03-28
"Previous lines: """" | files = directory_data.get('files', [] | # Filter and prepare file record | Line: file_records = []",code_seed.py,code_seed.py,file_records,variable,1581,2025-03-28
Previous lines: file_records = [ | for file_info in files | # Create a clean record with essential metadat | Line: record = {,code_seed.py,code_seed.py,record,variable,1584,2025-03-28
"Previous lines: file_records.append(record | return self.write_csv(file_records, filename | def generate_identifier_csv(self, directory_data: Dict[str, Any], | Line: filename: str = ""codeseed_identifiers.csv"") -> str:",code_seed.py,code_seed.py,str,variable,1617,2025-03-28
"Previous lines: Returns | Path to the created CSV file | """" | Line: files = directory_data.get('files', [])",code_seed.py,code_seed.py,files,variable,1632,2025-03-28
"Previous lines: """" | files = directory_data.get('files', [] | # Collect all identifiers across file | Line: identifier_records = []",code_seed.py,code_seed.py,identifier_records,variable,1635,2025-03-28
Previous lines: continu | # Process identifier | for identifier in file_info['identifiers'] | Line: record = {,code_seed.py,code_seed.py,record,variable,1643,2025-03-28
"Previous lines: identifier_records.append(record | return self.write_csv(identifier_records, filename | def generate_documentation_csv(self, directory_data: Dict[str, Any], | Line: filename: str = ""codeseed_documentation.csv"") -> str:",code_seed.py,code_seed.py,str,variable,1657,2025-03-28
"Previous lines: Returns | Path to the created CSV file | """" | Line: files = directory_data.get('files', [])",code_seed.py,code_seed.py,files,variable,1672,2025-03-28
"Previous lines: """" | files = directory_data.get('files', [] | # Collect all documentation across file | Line: documentation_records = []",code_seed.py,code_seed.py,documentation_records,variable,1675,2025-03-28
Previous lines: continu | # Process documentatio | for doc in file_info['documentation'] | Line: record = {,code_seed.py,code_seed.py,record,variable,1683,2025-03-28
"Previous lines: directory_analyzer: System for analyzing directory structures | output_manager: System for generating output files | """" | Line: def __init__(self, output_dir: str = None) -> None:",code_seed.py,code_seed.py,str,variable,1714,2025-03-28
"Previous lines: output_dir: Directory for output files | """" | # Initialize component system | Line: self.directory_analyzer = DirectoryAnalyzer()",code_seed.py,code_seed.py,directory_analyzer,variable,1726,2025-03-28
"Previous lines: """" | # Initialize component system | self.directory_analyzer = DirectoryAnalyzer( | Line: self.output_manager = OutputManager(output_dir)",code_seed.py,code_seed.py,output_manager,variable,1727,2025-03-28
"Previous lines: logger.debug(""CodeSeed initialized with component systems"" | def analyze_directory(self, directory: str, | exclude_patterns: List[str] = None | Line: output_prefix: str = ""codeseed"") -> Dict[str, str]:",code_seed.py,code_seed.py,str,variable,1733,2025-03-28
"Previous lines: """" | logger.info(f""Starting analysis of directory: {directory}"" | # Perform directory analysi | Line: directory_data = self.directory_analyzer.scan_directory(directory, exclude_patterns)",code_seed.py,code_seed.py,directory_data,variable,1752,2025-03-28
"Previous lines: # Perform directory analysi | directory_data = self.directory_analyzer.scan_directory(directory, exclude_patterns | # Generate output file | Line: output_files = {}",code_seed.py,code_seed.py,output_files,variable,1755,2025-03-28
Previous lines: # Generate output file | output_files = { | # File metadata CS | Line: file_csv = self.output_manager.generate_file_csv(,code_seed.py,code_seed.py,file_csv,variable,1758,2025-03-28
"Previous lines: directory_data, f""{output_prefix}_files.csv"" | output_files['files_csv'] = file_cs | # Identifier metadata CS | Line: identifier_csv = self.output_manager.generate_identifier_csv(",code_seed.py,code_seed.py,identifier_csv,variable,1763,2025-03-28
"Previous lines: directory_data, f""{output_prefix}_identifiers.csv"" | output_files['identifiers_csv'] = identifier_cs | # Documentation metadata CS | Line: documentation_csv = self.output_manager.generate_documentation_csv(",code_seed.py,code_seed.py,documentation_csv,variable,1768,2025-03-28
"Previous lines: """" | import argpars | # Set up argument parse | Line: parser = argparse.ArgumentParser(",code_seed.py,code_seed.py,parser,variable,1806,2025-03-28
"Previous lines: import argpars | # Set up argument parse | parser = argparse.ArgumentParser | Line: description=""CodeSeed: Extract rich code metadata for analysis in DataTrellis""",code_seed.py,code_seed.py,description,variable,1807,2025-03-28
"Previous lines: # Set up argument parse | parser = argparse.ArgumentParser | description=""CodeSeed: Extract rich code metadata for analysis in DataTrellis | Line: parser.add_argument('directory', nargs='?', default='.',",code_seed.py,code_seed.py,nargs,variable,1809,2025-03-28
"Previous lines: # Set up argument parse | parser = argparse.ArgumentParser | description=""CodeSeed: Extract rich code metadata for analysis in DataTrellis | Line: parser.add_argument('directory', nargs='?', default='.',",code_seed.py,code_seed.py,default,variable,1809,2025-03-28
"Previous lines: parser = argparse.ArgumentParser | description=""CodeSeed: Extract rich code metadata for analysis in DataTrellis | parser.add_argument('directory', nargs='?', default='.' | Line: help=""Directory to analyze (defaults to current directory)"")",code_seed.py,code_seed.py,help,variable,1810,2025-03-28
"Previous lines: description=""CodeSeed: Extract rich code metadata for analysis in DataTrellis | parser.add_argument('directory', nargs='?', default='.' | help=""Directory to analyze (defaults to current directory)"" | Line: parser.add_argument('--output-dir', '-o', default='.',",code_seed.py,code_seed.py,default,variable,1811,2025-03-28
"Previous lines: parser.add_argument('directory', nargs='?', default='.' | help=""Directory to analyze (defaults to current directory)"" | parser.add_argument('--output-dir', '-o', default='.' | Line: help=""Directory for output files (defaults to current directory)"")",code_seed.py,code_seed.py,help,variable,1812,2025-03-28
"Previous lines: help=""Directory to analyze (defaults to current directory)"" | parser.add_argument('--output-dir', '-o', default='.' | help=""Directory for output files (defaults to current directory)"" | Line: parser.add_argument('--prefix', '-p', default='codeseed',",code_seed.py,code_seed.py,default,variable,1813,2025-03-28
"Previous lines: parser.add_argument('--output-dir', '-o', default='.' | help=""Directory for output files (defaults to current directory)"" | parser.add_argument('--prefix', '-p', default='codeseed' | Line: help=""Prefix for output filenames"")",code_seed.py,code_seed.py,help,variable,1814,2025-03-28
"Previous lines: help=""Directory for output files (defaults to current directory)"" | parser.add_argument('--prefix', '-p', default='codeseed' | help=""Prefix for output filenames"" | Line: parser.add_argument('--exclude', '-e', action='append',",code_seed.py,code_seed.py,action,variable,1815,2025-03-28
"Previous lines: parser.add_argument('--prefix', '-p', default='codeseed' | help=""Prefix for output filenames"" | parser.add_argument('--exclude', '-e', action='append' | Line: help=""Patterns to exclude (can be specified multiple times)"")",code_seed.py,code_seed.py,help,variable,1816,2025-03-28
"Previous lines: help=""Prefix for output filenames"" | parser.add_argument('--exclude', '-e', action='append' | help=""Patterns to exclude (can be specified multiple times)"" | Line: parser.add_argument('--verbose', '-v', action='store_true',",code_seed.py,code_seed.py,action,variable,1817,2025-03-28
"Previous lines: parser.add_argument('--exclude', '-e', action='append' | help=""Patterns to exclude (can be specified multiple times)"" | parser.add_argument('--verbose', '-v', action='store_true' | Line: help=""Enable verbose logging"")",code_seed.py,code_seed.py,help,variable,1818,2025-03-28
"Previous lines: help=""Patterns to exclude (can be specified multiple times)"" | parser.add_argument('--verbose', '-v', action='store_true' | help=""Enable verbose logging"" | Line: parser.add_argument('--version', action='store_true',",code_seed.py,code_seed.py,action,variable,1819,2025-03-28
"Previous lines: parser.add_argument('--verbose', '-v', action='store_true' | help=""Enable verbose logging"" | parser.add_argument('--version', action='store_true' | Line: help=""Show version information"")",code_seed.py,code_seed.py,help,variable,1820,2025-03-28
"Previous lines: parser.add_argument('--version', action='store_true' | help=""Show version information"" | # Parse argument | Line: args = parser.parse_args()",code_seed.py,code_seed.py,args,variable,1823,2025-03-28
Previous lines: # Run analysi | try | # Initialize CodeSee | Line: code_seed = CodeSeed(args.output_dir),code_seed.py,code_seed.py,code_seed,variable,1837,2025-03-28
Previous lines: # Initialize CodeSee | code_seed = CodeSeed(args.output_dir | # Analyze director | Line: output_files = code_seed.analyze_directory(,code_seed.py,code_seed.py,output_files,variable,1840,2025-03-28
"Previous lines: import tracebac | logger.debug(traceback.format_exc() | sys.exit(1 | Line: if __name__ == ""__main__"":",code_seed.py,code_seed.py,__name__,variable,1857,2025-03-28
"Previous lines: This codebase serves as a reference implementation for clear semantic linkin | between identifiers, inline documentation, and formal docstrings | """""" | Line: import os",code_seed.py,code_seed.py,os,import,28,2025-03-28
"Previous lines: between identifiers, inline documentation, and formal docstrings | """""" | import o | Line: import sys",code_seed.py,code_seed.py,sys,import,29,2025-03-28
"Previous lines: """""" | import o | import sy | Line: import re",code_seed.py,code_seed.py,re,import,30,2025-03-28
Previous lines: import o | import sy | import r | Line: import csv,code_seed.py,code_seed.py,csv,import,31,2025-03-28
Previous lines: import sy | import r | import cs | Line: import json,code_seed.py,code_seed.py,json,import,32,2025-03-28
Previous lines: import r | import cs | import jso | Line: import time,code_seed.py,code_seed.py,time,import,33,2025-03-28
Previous lines: import cs | import jso | import tim | Line: import datetime,code_seed.py,code_seed.py,datetime,import,34,2025-03-28
Previous lines: import jso | import tim | import datetim | Line: import hashlib,code_seed.py,code_seed.py,hashlib,import,35,2025-03-28
Previous lines: import tim | import datetim | import hashli | Line: import logging,code_seed.py,code_seed.py,logging,import,36,2025-03-28
Previous lines: import datetim | import hashli | import loggin | Line: from pathlib import Path,code_seed.py,code_seed.py,Path,import,37,2025-03-28
"Previous lines: import hashli | import loggin | from pathlib import Pat | Line: from collections import defaultdict, Counter",code_seed.py,code_seed.py,defaultdict,import,38,2025-03-28
"Previous lines: import loggin | from pathlib import Pat | from collections import defaultdict, Counte | Line: from typing import Dict, List, Tuple, Set, Any, Optional, Union, Iterator",code_seed.py,code_seed.py,Dict,import,39,2025-03-28
"Previous lines: Provides a user-friendly interface to the CodeSeed system, allowin | easy analysis of code directories and generation of rich metadata | """" | Line: import argparse",code_seed.py,code_seed.py,argparse,import,1803,2025-03-28
"Previous lines: print(f""- {output_type}: {file_path}"" | except Exception as e | logger.error(f""Error during analysis: {e}"" | Line: import traceback",code_seed.py,code_seed.py,traceback,import,1852,2025-03-28
Previous lines: import datetim | import hashli | import loggin | Line: from pathlib import Path,code_seed.py,code_seed.py,pathlib,from_import,37,2025-03-28
"Previous lines: import hashli | import loggin | from pathlib import Pat | Line: from collections import defaultdict, Counter",code_seed.py,code_seed.py,collections,from_import,38,2025-03-28
"Previous lines: import loggin | from pathlib import Pat | from collections import defaultdict, Counte | Line: from typing import Dict, List, Tuple, Set, Any, Optional, Union, Iterator",code_seed.py,code_seed.py,typing,from_import,39,2025-03-28
"Previous lines: # Network of relationships between identifier | self.relationship_map: Dict[str, Set[str]] = defaultdict(set | logger.debug(""IdentifierTracker initialized with patterns for multiple languages"" | Line: def extract_identifiers(self, content: str, file_type: str) -> Dict[str, List[Dict[str, Any]]]:",code_seed.py,code_seed.py,content,parameter,170,2025-03-28
"Previous lines: # Network of relationships between identifier | self.relationship_map: Dict[str, Set[str]] = defaultdict(set | logger.debug(""IdentifierTracker initialized with patterns for multiple languages"" | Line: def extract_identifiers(self, content: str, file_type: str) -> Dict[str, List[Dict[str, Any]]]:",code_seed.py,code_seed.py,file_type,parameter,170,2025-03-28
"Previous lines: logger.debug(f""Extracted {sum(len(ids) for ids in identifiers.values())} identifiers | f""of {len(identifiers)} types from {language} content"" | return identifier | Line: def _determine_language(self, file_type: str) -> str:",code_seed.py,code_seed.py,file_type,parameter,235,2025-03-28
"Previous lines: return lan | # Default to a basic set of pattern | return 'python'  # Default to Python pattern | Line: def _extract_context(self, content: str, match: re.Match) -> str:",code_seed.py,code_seed.py,content,parameter,275,2025-03-28
"Previous lines: return lan | # Default to a basic set of pattern | return 'python'  # Default to Python pattern | Line: def _extract_context(self, content: str, match: re.Match) -> str:",code_seed.py,code_seed.py,match,parameter,275,2025-03-28
"Previous lines: if inline_comment | context.append(""Comment: "" + inline_comment | return "" | "".join(context | Line: def _update_relationships(self, content: str, id_name: str, match: re.Match) -> None:",code_seed.py,code_seed.py,content,parameter,333,2025-03-28
"Previous lines: if inline_comment | context.append(""Comment: "" + inline_comment | return "" | "".join(context | Line: def _update_relationships(self, content: str, id_name: str, match: re.Match) -> None:",code_seed.py,code_seed.py,id_name,parameter,333,2025-03-28
"Previous lines: if inline_comment | context.append(""Comment: "" + inline_comment | return "" | "".join(context | Line: def _update_relationships(self, content: str, id_name: str, match: re.Match) -> None:",code_seed.py,code_seed.py,match,parameter,333,2025-03-28
"Previous lines: 'question': r'\?{2,}',  # Multiple question marks often indicate uncertaint | 'emphasis': r'!{2,}',  # Multiple exclamation points indicate emphasi | logger.debug(""DocumentationExtractor initialized with patterns for multiple languages"" | Line: def extract_documentation(self, content: str, file_type: str) -> Dict[str, List[Dict[str, Any]]]:",code_seed.py,code_seed.py,content,parameter,472,2025-03-28
"Previous lines: 'question': r'\?{2,}',  # Multiple question marks often indicate uncertaint | 'emphasis': r'!{2,}',  # Multiple exclamation points indicate emphasi | logger.debug(""DocumentationExtractor initialized with patterns for multiple languages"" | Line: def extract_documentation(self, content: str, file_type: str) -> Dict[str, List[Dict[str, Any]]]:",code_seed.py,code_seed.py,file_type,parameter,472,2025-03-28
"Previous lines: logger.debug(f""Extracted {sum(len(docs) for docs in documentation.values())} documentation | f""elements of {len(documentation)} types from {language} content"" | return documentatio | Line: def _determine_language(self, file_type: str) -> str:",code_seed.py,code_seed.py,file_type,parameter,532,2025-03-28
"Previous lines: return lan | # Default to a basic set of pattern | return 'python'  # Default to Python pattern | Line: def _clean_doc_content(self, content: str) -> str:",code_seed.py,code_seed.py,content,parameter,572,2025-03-28
"Previous lines: content = re.sub(r'\s+', ' ', content | content = content.strip( | return conten | Line: def _extract_cognitive_markers(self, content: str) -> Dict[str, List[str]]:",code_seed.py,code_seed.py,content,parameter,606,2025-03-28
"Previous lines: marker_content = match.group(1) if match.groups() else match.group(0 | markers[marker_type].append(marker_content.strip() | return dict(markers | Line: def get_documentation_data(self, documentation: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:",code_seed.py,code_seed.py,documentation,parameter,631,2025-03-28
"Previous lines: marker_content = match.group(1) if match.groups() else match.group(0 | markers[marker_type].append(marker_content.strip() | return dict(markers | Line: def get_documentation_data(self, documentation: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:",code_seed.py,code_seed.py,List[Dict[str,parameter,631,2025-03-28
"Previous lines: marker_content = match.group(1) if match.groups() else match.group(0 | markers[marker_type].append(marker_content.strip() | return dict(markers | Line: def get_documentation_data(self, documentation: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:",code_seed.py,code_seed.py,Any]]],parameter,631,2025-03-28
"Previous lines: 'block_comments': r'/\*.*?\*/' | } | logger.debug(""PatternRecognizer initialized with pattern templates"" | Line: def recognize_patterns(self, content: str, file_type: str) -> Dict[str, int]:",code_seed.py,code_seed.py,content,parameter,758,2025-03-28
"Previous lines: 'block_comments': r'/\*.*?\*/' | } | logger.debug(""PatternRecognizer initialized with pattern templates"" | Line: def recognize_patterns(self, content: str, file_type: str) -> Dict[str, int]:",code_seed.py,code_seed.py,file_type,parameter,758,2025-03-28
"Previous lines: logger.debug(f""Recognized {sum(pattern_counts.values())} pattern instances | f""across {len(pattern_counts)} pattern types"" | return dict(pattern_counts | Line: def identify_signatures(self, content: str) -> Dict[str, Dict[str, int]]:",code_seed.py,code_seed.py,content,parameter,788,2025-03-28
"Previous lines: 'category': self._get_pattern_category(pattern) | } | return pattern_dat | Line: def _get_pattern_category(self, pattern: str) -> str:",code_seed.py,code_seed.py,pattern,parameter,843,2025-03-28
"Previous lines: self.doc_extractor = DocumentationExtractor( | self.pattern_recognizer = PatternRecognizer( | logger.debug(""FileAnalyzer initialized with specialized analyzers"" | Line: def analyze_file(self, file_path: str) -> Dict[str, Any]:",code_seed.py,code_seed.py,file_path,parameter,913,2025-03-28
"Previous lines: 'path': str(path_obj) | 'name': path_obj.name | 'error': str(e | Line: def _guess_mime_type(self, file_path: str) -> str:",code_seed.py,code_seed.py,file_path,parameter,1020,2025-03-28
"Previous lines: return 'application/octet-stream | # Defaul | return 'application/octet-stream | Line: def _is_text_file(self, mime_type: str) -> bool:",code_seed.py,code_seed.py,mime_type,parameter,1070,2025-03-28
"Previous lines: 'application/json' | 'application/xml' | 'application/x-yaml' | Line: def _hash_content(self, content: str) -> str:",code_seed.py,code_seed.py,content,parameter,1092,2025-03-28
"Previous lines: SHA-256 hash of content | """" | return hashlib.sha256(content.encode('utf-8')).hexdigest( | Line: def _extract_file_markers(self, content: str) -> Dict[str, List[str]]:",code_seed.py,code_seed.py,content,parameter,1108,2025-03-28
"Previous lines: marker_text = match.group(1) if match.groups() else match.group(0 | markers[marker_type].append(marker_text.strip() | return dict(markers | Line: def _flatten_identifiers(self, identifiers: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:",code_seed.py,code_seed.py,identifiers,parameter,1145,2025-03-28
"Previous lines: marker_text = match.group(1) if match.groups() else match.group(0 | markers[marker_type].append(marker_text.strip() | return dict(markers | Line: def _flatten_identifiers(self, identifiers: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:",code_seed.py,code_seed.py,List[Dict[str,parameter,1145,2025-03-28
"Previous lines: marker_text = match.group(1) if match.groups() else match.group(0 | markers[marker_type].append(marker_text.strip() | return dict(markers | Line: def _flatten_identifiers(self, identifiers: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:",code_seed.py,code_seed.py,Any]]],parameter,1145,2025-03-28
"Previous lines: 'context': identifier.get('context', '' | flattened.append(record | return flattene | Line: def _flatten_documentation(self, documentation: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:",code_seed.py,code_seed.py,documentation,parameter,1175,2025-03-28
"Previous lines: 'context': identifier.get('context', '' | flattened.append(record | return flattene | Line: def _flatten_documentation(self, documentation: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:",code_seed.py,code_seed.py,List[Dict[str,parameter,1175,2025-03-28
"Previous lines: 'context': identifier.get('context', '' | flattened.append(record | return flattene | Line: def _flatten_documentation(self, documentation: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:",code_seed.py,code_seed.py,Any]]],parameter,1175,2025-03-28
"Previous lines: # Track inaccessible directorie | self.inaccessible_dirs = [ | logger.debug(""DirectoryAnalyzer initialized with file analyzer"" | Line: def scan_directory(self, directory: str, exclude_patterns: List[str] = None) -> Dict[str, Any]:",code_seed.py,code_seed.py,directory,parameter,1238,2025-03-28
"Previous lines: # Track inaccessible directorie | self.inaccessible_dirs = [ | logger.debug(""DirectoryAnalyzer initialized with file analyzer"" | Line: def scan_directory(self, directory: str, exclude_patterns: List[str] = None) -> Dict[str, Any]:",code_seed.py,code_seed.py,exclude_patterns,parameter,1238,2025-03-28
"Previous lines: logger.info(f""Completed directory scan of {directory}"" | logger.info(f""Processed {dir_info['file_count']} files in {dir_info['elapsed_seconds']:.2f} seconds"" | return dir_inf | Line: def _mime_to_language(self, mime_type: str) -> str:",code_seed.py,code_seed.py,mime_type,parameter,1353,2025-03-28
"Previous lines: 'application/x-yaml': 'YAML' | 'text/x-shellscript': 'Shell Script' | return mime_map.get(mime_type, mime_type | Line: def _update_relationship_map(self, file_info: Dict[str, Any], current_dir: str, base_dir: str) -> None:",code_seed.py,code_seed.py,file_info,parameter,1383,2025-03-28
"Previous lines: 'application/x-yaml': 'YAML' | 'text/x-shellscript': 'Shell Script' | return mime_map.get(mime_type, mime_type | Line: def _update_relationship_map(self, file_info: Dict[str, Any], current_dir: str, base_dir: str) -> None:",code_seed.py,code_seed.py,Any],parameter,1383,2025-03-28
"Previous lines: 'application/x-yaml': 'YAML' | 'text/x-shellscript': 'Shell Script' | return mime_map.get(mime_type, mime_type | Line: def _update_relationship_map(self, file_info: Dict[str, Any], current_dir: str, base_dir: str) -> None:",code_seed.py,code_seed.py,current_dir,parameter,1383,2025-03-28
"Previous lines: 'application/x-yaml': 'YAML' | 'text/x-shellscript': 'Shell Script' | return mime_map.get(mime_type, mime_type | Line: def _update_relationship_map(self, file_info: Dict[str, Any], current_dir: str, base_dir: str) -> None:",code_seed.py,code_seed.py,base_dir,parameter,1383,2025-03-28
"Previous lines: Attributes | output_dir: Directory for output files | """" | Line: def __init__(self, output_dir: str = None) -> None:",code_seed.py,code_seed.py,output_dir,parameter,1479,2025-03-28
"Previous lines: # Ensure output directory exist | os.makedirs(self.output_dir, exist_ok=True | logger.debug(f""OutputManager initialized with output directory: {self.output_dir}"" | Line: def write_csv(self, data: List[Dict[str, Any]], filename: str, max_field_length: int = 32000) -> str:",code_seed.py,code_seed.py,data,parameter,1498,2025-03-28
"Previous lines: # Ensure output directory exist | os.makedirs(self.output_dir, exist_ok=True | logger.debug(f""OutputManager initialized with output directory: {self.output_dir}"" | Line: def write_csv(self, data: List[Dict[str, Any]], filename: str, max_field_length: int = 32000) -> str:",code_seed.py,code_seed.py,Any]],parameter,1498,2025-03-28
"Previous lines: # Ensure output directory exist | os.makedirs(self.output_dir, exist_ok=True | logger.debug(f""OutputManager initialized with output directory: {self.output_dir}"" | Line: def write_csv(self, data: List[Dict[str, Any]], filename: str, max_field_length: int = 32000) -> str:",code_seed.py,code_seed.py,filename,parameter,1498,2025-03-28
"Previous lines: # Ensure output directory exist | os.makedirs(self.output_dir, exist_ok=True | logger.debug(f""OutputManager initialized with output directory: {self.output_dir}"" | Line: def write_csv(self, data: List[Dict[str, Any]], filename: str, max_field_length: int = 32000) -> str:",code_seed.py,code_seed.py,max_field_length,parameter,1498,2025-03-28
"Previous lines: writer.writerow(processed_record | logger.info(f""Wrote {len(data)} records to {output_path}"" | return output_pat | Line: def generate_file_csv(self, directory_data: Dict[str, Any], filename: str = ""codeseed_files.csv"") -> str:",code_seed.py,code_seed.py,directory_data,parameter,1563,2025-03-28
"Previous lines: writer.writerow(processed_record | logger.info(f""Wrote {len(data)} records to {output_path}"" | return output_pat | Line: def generate_file_csv(self, directory_data: Dict[str, Any], filename: str = ""codeseed_files.csv"") -> str:",code_seed.py,code_seed.py,Any],parameter,1563,2025-03-28
"Previous lines: writer.writerow(processed_record | logger.info(f""Wrote {len(data)} records to {output_path}"" | return output_pat | Line: def generate_file_csv(self, directory_data: Dict[str, Any], filename: str = ""codeseed_files.csv"") -> str:",code_seed.py,code_seed.py,filename,parameter,1563,2025-03-28
"Previous lines: record[f'marker_{marker}_sample'] = instances[0 | file_records.append(record | return self.write_csv(file_records, filename | Line: def generate_identifier_csv(self, directory_data: Dict[str, Any], 
                             filename: str = ""codeseed_identifiers.csv"") -> str:",code_seed.py,code_seed.py,directory_data,parameter,1616,2025-03-28
"Previous lines: record[f'marker_{marker}_sample'] = instances[0 | file_records.append(record | return self.write_csv(file_records, filename | Line: def generate_identifier_csv(self, directory_data: Dict[str, Any], 
                             filename: str = ""codeseed_identifiers.csv"") -> str:",code_seed.py,code_seed.py,Any],parameter,1616,2025-03-28
"Previous lines: record[f'marker_{marker}_sample'] = instances[0 | file_records.append(record | return self.write_csv(file_records, filename | Line: def generate_identifier_csv(self, directory_data: Dict[str, Any], 
                             filename: str = ""codeseed_identifiers.csv"") -> str:",code_seed.py,code_seed.py,filename,parameter,1616,2025-03-28
"Previous lines: 'modified_date': file_info.get('modified_date', '') | identifier_records.append(record | return self.write_csv(identifier_records, filename | Line: def generate_documentation_csv(self, directory_data: Dict[str, Any], 
                                filename: str = ""codeseed_documentation.csv"") -> str:",code_seed.py,code_seed.py,directory_data,parameter,1656,2025-03-28
"Previous lines: 'modified_date': file_info.get('modified_date', '') | identifier_records.append(record | return self.write_csv(identifier_records, filename | Line: def generate_documentation_csv(self, directory_data: Dict[str, Any], 
                                filename: str = ""codeseed_documentation.csv"") -> str:",code_seed.py,code_seed.py,Any],parameter,1656,2025-03-28
"Previous lines: 'modified_date': file_info.get('modified_date', '') | identifier_records.append(record | return self.write_csv(identifier_records, filename | Line: def generate_documentation_csv(self, directory_data: Dict[str, Any], 
                                filename: str = ""codeseed_documentation.csv"") -> str:",code_seed.py,code_seed.py,filename,parameter,1656,2025-03-28
"Previous lines: directory_analyzer: System for analyzing directory structures | output_manager: System for generating output files | """" | Line: def __init__(self, output_dir: str = None) -> None:",code_seed.py,code_seed.py,output_dir,parameter,1714,2025-03-28
"Previous lines: self.directory_analyzer = DirectoryAnalyzer( | self.output_manager = OutputManager(output_dir | logger.debug(""CodeSeed initialized with component systems"" | Line: def analyze_directory(self, directory: str, 
                        exclude_patterns: List[str] = None,
                        output_prefix: str = ""codeseed"") -> Dict[str, str]:",code_seed.py,code_seed.py,directory,parameter,1731,2025-03-28
"Previous lines: self.directory_analyzer = DirectoryAnalyzer( | self.output_manager = OutputManager(output_dir | logger.debug(""CodeSeed initialized with component systems"" | Line: def analyze_directory(self, directory: str, 
                        exclude_patterns: List[str] = None,
                        output_prefix: str = ""codeseed"") -> Dict[str, str]:",code_seed.py,code_seed.py,exclude_patterns,parameter,1731,2025-03-28
"Previous lines: self.directory_analyzer = DirectoryAnalyzer( | self.output_manager = OutputManager(output_dir | logger.debug(""CodeSeed initialized with component systems"" | Line: def analyze_directory(self, directory: str, 
                        exclude_patterns: List[str] = None,
                        output_prefix: str = ""codeseed"") -> Dict[str, str]:",code_seed.py,code_seed.py,output_prefix,parameter,1731,2025-03-28
"Previous lines: output_files['documentation_csv'] = documentation_cs | logger.info(f""Analysis complete. Output files generated in: {self.output_manager.output_dir}"" | return output_file | Line: def analyze_file(self, file_path: str) -> Dict[str, Any]:",code_seed.py,code_seed.py,file_path,parameter,1776,2025-03-28
"Previous lines: used throughout the analysis process | """" | # Language-specific identifier pattern | Line: self.patterns = {",code_seed.py,code_seed.py,patterns,attribute,112,2025-03-28
"Previous lines: 'code_block': r'```([a-zA-Z0-9_]*)$' | 'emphasis': r'\*\*([^\*]+)\*\*' | # Storage for identifier context | Line: self.contexts: Dict[str, Dict[str, Any]] = {}",code_seed.py,code_seed.py,contexts,attribute,163,2025-03-28
"Previous lines: # Storage for identifier context | self.contexts: Dict[str, Dict[str, Any]] = { | # Network of relationships between identifier | Line: self.relationship_map: Dict[str, Set[str]] = defaultdict(set)",code_seed.py,code_seed.py,relationship_map,attribute,166,2025-03-28
"Previous lines: Returns | Dictionary mapping identifier types to lists of found identifiers | """" | Line: language = self._determine_language(file_type)",code_seed.py,code_seed.py,_determine_language,attribute,185,2025-03-28
"Previous lines: Dictionary mapping identifier types to lists of found identifiers | """" | language = self._determine_language(file_type | Line: patterns = self.patterns.get(language, {})",code_seed.py,code_seed.py,patterns,attribute,186,2025-03-28
"Previous lines: 'type': 'parameter' | 'line': content.count('\n', 0, match.start()) + 1 | 'pos': match.start() | Line: 'context': self._extract_context(content, match)",code_seed.py,code_seed.py,_extract_context,attribute,213,2025-03-28
"Previous lines: 'type': id_type | 'line': content.count('\n', 0, match.start()) + 1 | 'pos': match.start() | Line: 'context': self._extract_context(content, match)",code_seed.py,code_seed.py,_extract_context,attribute,224,2025-03-28
"Previous lines: 'context': self._extract_context(content, match | identifiers[id_type].append(identifier | # Update relationship map for this identifie | Line: self._update_relationships(content, id_name, match)",code_seed.py,code_seed.py,_update_relationships,attribute,229,2025-03-28
"Previous lines: scope_end = len(content | scope = content[scope_start:scope_end | # Find other identifiers in this scop | Line: for pattern_type, patterns in self.patterns.items():",code_seed.py,code_seed.py,patterns,attribute,363,2025-03-28
Previous lines: other_id = other_match.group(1 | if other_id != id_name and len(other_id) > 2 | # Add bidirectional relationshi | Line: self.relationship_map[id_name].add(other_id),code_seed.py,code_seed.py,relationship_map,attribute,370,2025-03-28
Previous lines: if other_id != id_name and len(other_id) > 2 | # Add bidirectional relationshi | self.relationship_map[id_name].add(other_id | Line: self.relationship_map[other_id].add(id_name),code_seed.py,code_seed.py,relationship_map,attribute,371,2025-03-28
"Previous lines: """" | identifier_data = [ | # Process each identifier typ | Line: for id_type, identifiers in self.contexts.items():",code_seed.py,code_seed.py,contexts,attribute,387,2025-03-28
"Previous lines: 'type': identifier['type'] | 'line': identifier['line'] | 'context': identifier['context'] | Line: 'related': ','.join(self.relationship_map.get(identifier['name'], set())),",code_seed.py,code_seed.py,relationship_map,attribute,395,2025-03-28
"Previous lines: 'line': identifier['line'] | 'context': identifier['context'] | 'related': ','.join(self.relationship_map.get(identifier['name'], set())) | Line: 'relationship_count': len(self.relationship_map.get(identifier['name'], set()))",code_seed.py,code_seed.py,relationship_map,attribute,396,2025-03-28
"Previous lines: of documentation, from formal docstrings to quick notes and TODOs | """" | # Patterns for documentation extractio | Line: self.doc_patterns = {",code_seed.py,code_seed.py,doc_patterns,attribute,427,2025-03-28
Previous lines: 'markdown': | 'code_comment': r'<!--(.*?)-->' | # Patterns for identifying cognitive marker | Line: self.cognitive_markers = {,code_seed.py,code_seed.py,cognitive_markers,attribute,458,2025-03-28
"Previous lines: Returns | Dictionary mapping documentation types to lists of extracted elements | """" | Line: language = self._determine_language(file_type)",code_seed.py,code_seed.py,_determine_language,attribute,487,2025-03-28
"Previous lines: Dictionary mapping documentation types to lists of extracted elements | """" | language = self._determine_language(file_type | Line: patterns = self.doc_patterns.get(language, {})",code_seed.py,code_seed.py,doc_patterns,attribute,488,2025-03-28
"Previous lines: matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL | for match in matches | doc_content = match.group(1) if match.groups() else "" | Line: doc_content = self._clean_doc_content(doc_content)",code_seed.py,code_seed.py,_clean_doc_content,attribute,499,2025-03-28
"Previous lines: 'content': doc_content | 'line': content.count('\n', 0, match.start()) + 1 | 'length': len(doc_content) | Line: 'markers': self._extract_cognitive_markers(doc_content)",code_seed.py,code_seed.py,_extract_cognitive_markers,attribute,511,2025-03-28
"Previous lines: 'markers': self._extract_cognitive_markers(doc_content | documentation[doc_type].append(doc_record | # Extract cognitive markers across all conten | Line: for marker_type, pattern in self.cognitive_markers.items():",code_seed.py,code_seed.py,cognitive_markers,attribute,516,2025-03-28
"Previous lines: """" | markers: Dict[str, List[str]] = defaultdict(list | # Check for each marker typ | Line: for marker_type, pattern in self.cognitive_markers.items():",code_seed.py,code_seed.py,cognitive_markers,attribute,622,2025-03-28
"Previous lines: coding patterns, architectural approaches, and design signatures | """" | # Common code pattern templates | Line: self.code_patterns = {",code_seed.py,code_seed.py,code_patterns,attribute,692,2025-03-28
Previous lines: 'assert_statement': r'assert\s+' | 'mock_setup': r'mock\s*\(' | # Counter for pattern occurrence | Line: self.pattern_frequencies = Counter(),code_seed.py,code_seed.py,pattern_frequencies,attribute,730,2025-03-28
Previous lines: # Counter for pattern occurrence | self.pattern_frequencies = Counter( | # Code signature elements to identif | Line: self.signature_elements = {,code_seed.py,code_seed.py,signature_elements,attribute,733,2025-03-28
"Previous lines: """" | pattern_counts = Counter( | # Apply each patter | Line: for pattern_name, pattern in self.code_patterns.items():",code_seed.py,code_seed.py,code_patterns,attribute,776,2025-03-28
Previous lines: count = sum(1 for _ in matches | if count > 0 | pattern_counts[pattern_name] = coun | Line: self.pattern_frequencies[pattern_name] += count,code_seed.py,code_seed.py,pattern_frequencies,attribute,782,2025-03-28
"Previous lines: """" | signatures = { | # Check each signature element categor | Line: for category, elements in self.signature_elements.items():",code_seed.py,code_seed.py,signature_elements,attribute,805,2025-03-28
"Previous lines: """" | pattern_data = [ | # Get the top pattern | Line: for pattern, count in self.pattern_frequencies.most_common():",code_seed.py,code_seed.py,pattern_frequencies,attribute,834,2025-03-28
"Previous lines: pattern_data.append( | 'pattern': pattern | 'count': count | Line: 'category': self._get_pattern_category(pattern),",code_seed.py,code_seed.py,_get_pattern_category,attribute,838,2025-03-28
"Previous lines: approach to understanding code structure and patterns | """" | # Initialize analysis component | Line: self.identifier_tracker = IdentifierTracker()",code_seed.py,code_seed.py,identifier_tracker,attribute,907,2025-03-28
"Previous lines: """" | # Initialize analysis component | self.identifier_tracker = IdentifierTracker( | Line: self.doc_extractor = DocumentationExtractor()",code_seed.py,code_seed.py,doc_extractor,attribute,908,2025-03-28
Previous lines: # Initialize analysis component | self.identifier_tracker = IdentifierTracker( | self.doc_extractor = DocumentationExtractor( | Line: self.pattern_recognizer = PatternRecognizer(),code_seed.py,code_seed.py,pattern_recognizer,attribute,909,2025-03-28
Previous lines: 'modified_date': datetime.datetime.fromtimestamp(file_stats.st_mtime).strftime('%Y-%m-%d') | 'created_date': datetime.datetime.fromtimestamp(file_stats.st_ctime).strftime('%Y-%m-%d') | # Determine if this is a text file we should analyze in detai | Line: mime_type = self._guess_mime_type(file_path),code_seed.py,code_seed.py,_guess_mime_type,attribute,946,2025-03-28
"Previous lines: mime_type = self._guess_mime_type(file_path | file_info['mime_type'] = mime_typ | # For text files, perform deep analysi | Line: if self._is_text_file(mime_type) and file_stats.st_size < 1000000:  # Limit to 1MB | Comment: # Limit to 1MB",code_seed.py,code_seed.py,_is_text_file,attribute,950,2025-03-28
"Previous lines: 'line_count': len(lines) | 'avg_line_length': sum(len(line) for line in lines) / max(len(lines), 1) | 'empty_line_count': sum(1 for line in lines if not line.strip()) | Line: 'content_hash': self._hash_content(content),",code_seed.py,code_seed.py,_hash_content,attribute,962,2025-03-28
"Previous lines: 'content_hash': self._hash_content(content) | } | # Extract identifier | Line: identifiers = self.identifier_tracker.extract_identifiers(content, mime_type)",code_seed.py,code_seed.py,identifier_tracker,attribute,966,2025-03-28
"Previous lines: file_info['identifier_counts'] = {k: len(v) for k, v in identifiers.items() | file_info['total_identifiers'] = sum(len(v) for v in identifiers.values() | # Extract documentatio | Line: documentation = self.doc_extractor.extract_documentation(content, mime_type)",code_seed.py,code_seed.py,doc_extractor,attribute,971,2025-03-28
"Previous lines: file_info['documentation_counts'] = {k: len(v) for k, v in documentation.items() | file_info['total_documentation'] = sum(len(v) for v in documentation.values() | # Recognize pattern | Line: patterns = self.pattern_recognizer.recognize_patterns(content, mime_type)",code_seed.py,code_seed.py,pattern_recognizer,attribute,976,2025-03-28
Previous lines: file_info['pattern_counts'] = pattern | file_info['total_patterns'] = sum(patterns.values() | # Identify code signature | Line: signatures = self.pattern_recognizer.identify_signatures(content),code_seed.py,code_seed.py,pattern_recognizer,attribute,981,2025-03-28
Previous lines: else | file_info['documentation_density'] = | # Extract cognitive marker | Line: cognitive_markers = self._extract_file_markers(content),code_seed.py,code_seed.py,_extract_file_markers,attribute,991,2025-03-28
Previous lines: if cognitive_markers | file_info['cognitive_markers'] = cognitive_marker | # Create enriched metadat | Line: file_info['identifiers'] = self._flatten_identifiers(identifiers),code_seed.py,code_seed.py,_flatten_identifiers,attribute,996,2025-03-28
Previous lines: file_info['cognitive_markers'] = cognitive_marker | # Create enriched metadat | file_info['identifiers'] = self._flatten_identifiers(identifiers | Line: file_info['documentation'] = self._flatten_documentation(documentation),code_seed.py,code_seed.py,_flatten_documentation,attribute,997,2025-03-28
Previous lines: file_info['analysis_error'] = str(e | else | file_info['skipped_content_analysis'] = Tru | Line: if not self._is_text_file(mime_type):,code_seed.py,code_seed.py,_is_text_file,attribute,1004,2025-03-28
"Previous lines: comprehensive approach to mapping the code forest | """" | # Initialize file analyzer componen | Line: self.file_analyzer = FileAnalyzer()",code_seed.py,code_seed.py,file_analyzer,attribute,1228,2025-03-28
Previous lines: # Initialize file analyzer componen | self.file_analyzer = FileAnalyzer( | # Map of file relationship | Line: self.relationship_map = defaultdict(set),code_seed.py,code_seed.py,relationship_map,attribute,1231,2025-03-28
Previous lines: # Map of file relationship | self.relationship_map = defaultdict(set | # Track inaccessible directorie | Line: self.inaccessible_dirs = [],code_seed.py,code_seed.py,inaccessible_dirs,attribute,1234,2025-03-28
"Previous lines: continu | # Check if directory is accessibl | if not os.access(root, os.R_OK) | Line: self.inaccessible_dirs.append(root)",code_seed.py,code_seed.py,inaccessible_dirs,attribute,1285,2025-03-28
Previous lines: continu | # Analyze fil | try | Line: file_info = self.file_analyzer.analyze_file(file_path),code_seed.py,code_seed.py,file_analyzer,attribute,1304,2025-03-28
"Previous lines: dir_info['file_extensions'][extension] = dir_info['file_extensions'].get(extension, 0) + | # Update language statistics based on MIME typ | mime_type = file_info.get('mime_type', '' | Line: language = self._mime_to_language(mime_type)",code_seed.py,code_seed.py,_mime_to_language,attribute,1317,2025-03-28
"Previous lines: if language | dir_info['language_breakdown'][language] = dir_info['language_breakdown'].get(language, 0) + | # Map file relationship | Line: self._update_relationship_map(file_info, root, directory)",code_seed.py,code_seed.py,_update_relationship_map,attribute,1322,2025-03-28
Previous lines: dir_info['end_time'] = time.time( | dir_info['elapsed_seconds'] = dir_info['end_time'] - dir_info['start_time' | # Add relationship dat | Line: dir_info['file_relationships'] = self._get_relationship_data(),code_seed.py,code_seed.py,_get_relationship_data,attribute,1340,2025-03-28
Previous lines: # Add relationship dat | dir_info['file_relationships'] = self._get_relationship_data( | # Extract cross-file pattern | Line: dir_info['pattern_summary'] = self._extract_pattern_summary(),code_seed.py,code_seed.py,_extract_pattern_summary,attribute,1343,2025-03-28
Previous lines: # Extract cross-file pattern | dir_info['pattern_summary'] = self._extract_pattern_summary( | # Count inaccessible directorie | Line: dir_info['inaccessible_dirs'] = len(self.inaccessible_dirs),code_seed.py,code_seed.py,inaccessible_dirs,attribute,1346,2025-03-28
"Previous lines: other_path = os.path.join(current_dir, other_file | if os.path.isfile(other_path) and other_path != file_path | # Add relationship based on shared director | Line: self.relationship_map[file_path].add(other_path)",code_seed.py,code_seed.py,relationship_map,attribute,1407,2025-03-28
Previous lines: if os.path.isfile(other_path) and other_path != file_path | # Add relationship based on shared director | self.relationship_map[file_path].add(other_path | Line: self.relationship_map[other_path].add(file_path),code_seed.py,code_seed.py,relationship_map,attribute,1408,2025-03-28
Previous lines: base_name = os.path.splitext(file_name)[0 | if base_name | # Look for files with similar names in the entire directory tre | Line: for other_info in self.file_analyzer.pattern_recognizer.pattern_frequencies.keys():,code_seed.py,code_seed.py,file_analyzer,attribute,1416,2025-03-28
Previous lines: for other_info in self.file_analyzer.pattern_recognizer.pattern_frequencies.keys() | if base_name in other_info and other_info != file_path | # Add relationship based on name similarit | Line: self.relationship_map[file_path].add(other_info),code_seed.py,code_seed.py,relationship_map,attribute,1419,2025-03-28
Previous lines: if base_name in other_info and other_info != file_path | # Add relationship based on name similarit | self.relationship_map[file_path].add(other_info | Line: self.relationship_map[other_info].add(file_path),code_seed.py,code_seed.py,relationship_map,attribute,1420,2025-03-28
"Previous lines: """" | relationships = { | # Convert set values to lists for JSON serializatio | Line: for file_path, related_files in self.relationship_map.items():",code_seed.py,code_seed.py,relationship_map,attribute,1439,2025-03-28
"Previous lines: Dictionary containing pattern summary data | """" | # Get dominant pattern | Line: dominant_patterns = self.file_analyzer.pattern_recognizer.get_dominant_patterns()",code_seed.py,code_seed.py,file_analyzer,attribute,1456,2025-03-28
"Previous lines: output_dir: Directory for output files (defaults to current directory) | """" | # Set output director | Line: self.output_dir = output_dir or '.'",code_seed.py,code_seed.py,output_dir,attribute,1491,2025-03-28
"Previous lines: # Set output director | self.output_dir = output_dir or '. | # Ensure output directory exist | Line: os.makedirs(self.output_dir, exist_ok=True)",code_seed.py,code_seed.py,output_dir,attribute,1494,2025-03-28
"Previous lines: self.output_dir = output_dir or '. | # Ensure output directory exist | os.makedirs(self.output_dir, exist_ok=True | Line: logger.debug(f""OutputManager initialized with output directory: {self.output_dir}"")",code_seed.py,code_seed.py,output_dir,attribute,1496,2025-03-28
"Previous lines: logger.warning(""No data to write to CSV"" | return "" | # Prepare output pat | Line: output_path = os.path.join(self.output_dir, filename)",code_seed.py,code_seed.py,output_dir,attribute,1520,2025-03-28
"Previous lines: if instances and len(instances) > 0 | record[f'marker_{marker}_sample'] = instances[0 | file_records.append(record | Line: return self.write_csv(file_records, filename)",code_seed.py,code_seed.py,write_csv,attribute,1614,2025-03-28
"Previous lines: 'context': identifier.get('context', '') | 'modified_date': file_info.get('modified_date', '') | identifier_records.append(record | Line: return self.write_csv(identifier_records, filename)",code_seed.py,code_seed.py,write_csv,attribute,1654,2025-03-28
"Previous lines: 'has_markers': doc.get('has_markers', False) | 'modified_date': file_info.get('modified_date', '') | documentation_records.append(record | Line: return self.write_csv(documentation_records, filename)",code_seed.py,code_seed.py,write_csv,attribute,1694,2025-03-28
"Previous lines: output_dir: Directory for output files | """" | # Initialize component system | Line: self.directory_analyzer = DirectoryAnalyzer()",code_seed.py,code_seed.py,directory_analyzer,attribute,1726,2025-03-28
"Previous lines: """" | # Initialize component system | self.directory_analyzer = DirectoryAnalyzer( | Line: self.output_manager = OutputManager(output_dir)",code_seed.py,code_seed.py,output_manager,attribute,1727,2025-03-28
"Previous lines: """" | logger.info(f""Starting analysis of directory: {directory}"" | # Perform directory analysi | Line: directory_data = self.directory_analyzer.scan_directory(directory, exclude_patterns)",code_seed.py,code_seed.py,directory_analyzer,attribute,1752,2025-03-28
Previous lines: # Generate output file | output_files = { | # File metadata CS | Line: file_csv = self.output_manager.generate_file_csv(,code_seed.py,code_seed.py,output_manager,attribute,1758,2025-03-28
"Previous lines: directory_data, f""{output_prefix}_files.csv"" | output_files['files_csv'] = file_cs | # Identifier metadata CS | Line: identifier_csv = self.output_manager.generate_identifier_csv(",code_seed.py,code_seed.py,output_manager,attribute,1763,2025-03-28
"Previous lines: directory_data, f""{output_prefix}_identifiers.csv"" | output_files['identifiers_csv'] = identifier_cs | # Documentation metadata CS | Line: documentation_csv = self.output_manager.generate_documentation_csv(",code_seed.py,code_seed.py,output_manager,attribute,1768,2025-03-28
"Previous lines: documentation_csv = self.output_manager.generate_documentation_csv | directory_data, f""{output_prefix}_documentation.csv"" | output_files['documentation_csv'] = documentation_cs | Line: logger.info(f""Analysis complete. Output files generated in: {self.output_manager.output_dir}"")",code_seed.py,code_seed.py,output_manager,attribute,1772,2025-03-28
"Previous lines: Returns | Dictionary containing comprehensive file metadata | """" | Line: return self.directory_analyzer.file_analyzer.analyze_file(file_path)",code_seed.py,code_seed.py,directory_analyzer,attribute,1789,2025-03-28
Line: # 🌱 CodeSeed: Semantic Forest Mapper | Comment: # 🌱 CodeSeed: Semantic Forest Mapper,README.md,README.md,🌱 CodeSeed: Semantic Forest Mapper,heading,1,2025-03-28
"Previous lines: --accessibility-level: ""progressive-revelation"" | --relationship-emphasis: ""connections-over-components"" | ``` | Line: ## 🌲 The Cognitive Forest: A Mental Model | Comment: ## 🌲 The Cognitive Forest: A Mental Model",README.md,README.md,🌲 The Cognitive Forest: A Mental Model,heading,16,2025-03-28
"Previous lines: - **Seeds** (Patterns): Reusable concepts that can grow into new structure | - **Growth Rings** (Documentation): Historical records that tell the story of evolution | This forest metaphor isn't just descriptive—it's a cognitive tool that transforms how you understand code relationships, making complex systems more approachable through familiar natural patterns. | Line: ## 🧭 Navigational Guide: Maps for Exploration | Comment: ## 🧭 Navigational Guide: Maps for Exploration",README.md,README.md,🧭 Navigational Guide: Maps for Exploration,heading,29,2025-03-28
"Previous lines: 2. **🍃 Leaf Patterns (`identifiers.csv`)**: A detailed catalog of all identifiers—the leaves through which your code breathes—with their contexts, relationships, and semantic meanings | 3. **📜 Forest Chronicles (`documentation.csv`)**: An anthology of all documentation—the stories your code tells about itself—revealing intentions, explanations, and cognitive markers | Each map is designed as a rich CSV dataset ready for exploration in DataTrellis, allowing you to filter, sort, and visualize your codebase in ways that reveal its hidden structures and intentions. | Line: ## 🛠️ Practical Usage Guide | Comment: ## 🛠️ Practical Usage Guide",README.md,README.md,🛠️ Practical Usage Guide,heading,74,2025-03-28
"Previous lines: 3. **📜 Forest Chronicles (`documentation.csv`)**: An anthology of all documentation—the stories your code tells about itself—revealing intentions, explanations, and cognitive markers | Each map is designed as a rich CSV dataset ready for exploration in DataTrellis, allowing you to filter, sort, and visualize your codebase in ways that reveal its hidden structures and intentions. | ## 🛠️ Practical Usage Guide | Line: ### Installation | Comment: ### Installation",README.md,README.md,Installation,heading,76,2025-03-28
Previous lines: ## 🛠️ Practical Usage Guide | ### Installation | ```bas | Line: # Clone the repository | Comment: # Clone the repository,README.md,README.md,Clone the repository,heading,79,2025-03-28
Previous lines: ```bas | # Clone the repositor | git clone https://github.com/yourusername/codeseed.git | Line: # Navigate to the directory | Comment: # Navigate to the directory,README.md,README.md,Navigate to the directory,heading,82,2025-03-28
Previous lines: git clone https://github.com/yourusername/codeseed.git | # Navigate to the director | cd codeseed | Line: # Make the script executable | Comment: # Make the script executable,README.md,README.md,Make the script executable,heading,85,2025-03-28
Previous lines: # Make the script executabl | chmod +x codeseed.p | ``` | Line: ### Basic Usage | Comment: ### Basic Usage,README.md,README.md,Basic Usage,heading,89,2025-03-28
Previous lines: ``` | ### Basic Usage | ```bas | Line: # Analyze the current directory | Comment: # Analyze the current directory,README.md,README.md,Analyze the current directory,heading,92,2025-03-28
Previous lines: ```bas | # Analyze the current director | ./codeseed.py | Line: # Analyze a specific directory | Comment: # Analyze a specific directory,README.md,README.md,Analyze a specific directory,heading,95,2025-03-28
Previous lines: ./codeseed.py | # Analyze a specific director | ./codeseed.py /path/to/your/project | Line: # Specify output directory | Comment: # Specify output directory,README.md,README.md,Specify output directory,heading,98,2025-03-28
Previous lines: ./codeseed.py /path/to/your/project | # Specify output director | ./codeseed.py --output-dir ./analysis | Line: # Exclude patterns | Comment: # Exclude patterns,README.md,README.md,Exclude patterns,heading,101,2025-03-28
"Previous lines: # Exclude pattern | ./codeseed.py --exclude ""node_modules"" --exclude "".git | ``` | Line: ### Opening in DataTrellis | Comment: ### Opening in DataTrellis",README.md,README.md,Opening in DataTrellis,heading,105,2025-03-28
Previous lines: 2. Open DataTrelli | 3. Drag and drop the generated CSV file | 4. Begin exploring your codebase through multiple lenses | Line: ## 🌿 The D.A.T.A. Framework in Action | Comment: ## 🌿 The D.A.T.A. Framework in Action,README.md,README.md,🌿 The D.A.T.A. Framework in Action,heading,112,2025-03-28
Previous lines: 4. Begin exploring your codebase through multiple lenses | ## 🌿 The D.A.T.A. Framework in Action | CodeSeed implements the D.A.T.A. framework's principles to make code comprehension more accessible: | Line: ### Demystification | Comment: ### Demystification,README.md,README.md,Demystification,heading,116,2025-03-28
"Previous lines: CodeSeed implements the D.A.T.A. framework's principles to make code comprehension more accessible: | ### Demystification | CodeSeed transforms abstract code structures into intuitive forest metaphors. Instead of seeing disconnected files and functions, you'll see a living ecosystem where each element plays a recognizable role—making complex architectures immediately more approachable. | Line: ### Adaptive Interfaces | Comment: ### Adaptive Interfaces",README.md,README.md,Adaptive Interfaces,heading,120,2025-03-28
Previous lines: - Visual thinkers might start with relationship pattern | - Detail-oriented minds might begin with identifier catalog | - Big-picture thinkers might explore documentation first | Line: ### Transparent Complexity | Comment: ### Transparent Complexity,README.md,README.md,Transparent Complexity,heading,127,2025-03-28
"Previous lines: - Big-picture thinkers might explore documentation first | ### Transparent Complexity | CodeSeed doesn't hide complexity—it organizes it into digestible layers. Each map reveals a different dimension of your codebase, allowing you to progressively explore deeper levels of detail as your understanding grows. | Line: ### Empowerment through Exploration | Comment: ### Empowerment through Exploration",README.md,README.md,Empowerment through Exploration,heading,131,2025-03-28
"Previous lines: CodeSeed doesn't hide complexity—it organizes it into digestible layers. Each map reveals a different dimension of your codebase, allowing you to progressively explore deeper levels of detail as your understanding grows. | ### Empowerment through Exploration | By making code relationships explicit and navigable, CodeSeed transforms passive reading into active exploration. Users naturally discover patterns and connections as they move through the forest, building a mental model that grows increasingly sophisticated. | Line: ## 🌐 META4 Alignment | Comment: ## 🌐 META4 Alignment",README.md,README.md,🌐 META4 Alignment,heading,135,2025-03-28
Previous lines: - **T**emplate: Creating structured representations of code element | - **A**ssembly: Combining individual analyses into coherent system understandin | - **4** (Metaphor): Bridging technical code structure with intuitive forest metaphors | Line: ## 🔍 Semantic Bridge Protocol | Comment: ## 🔍 Semantic Bridge Protocol,README.md,README.md,🔍 Semantic Bridge Protocol,heading,145,2025-03-28
"Previous lines: 4. **Patterns ↔ Instances**: ""How do architectural choices manifest in specific files? | - Connect pattern counts in `files.csv` with specific implementations | These bridges help maintain orientation as you move between different perspectives, ensuring you always have a path back to familiar territory. | Line: ## 🌱 Growing Your Understanding | Comment: ## 🌱 Growing Your Understanding",README.md,README.md,🌱 Growing Your Understanding,heading,163,2025-03-28
"Previous lines: CodeSeed isn't just a tool—it's a living approach to code comprehension that grows alongside your understanding. As you explore your first code forest, you'll naturally develop new questions and insights that can be addressed through increasingly sophisticated analysis. | The generated CSVs serve as your first maps, but the true value comes from the mental models you build as you traverse these maps. Each exploration session will reveal new patterns and relationships, gradually transforming an intimidating codebase into a familiar landscape where you can navigate confidently. | --- | Line: ## 📊 Contextual Understanding Anchors | Comment: ## 📊 Contextual Understanding Anchors",README.md,README.md,📊 Contextual Understanding Anchors,heading,171,2025-03-28
"Previous lines: * | ``` | These anchors establish shared understanding for discussing, extending, and applying CodeSeed concepts across different contexts. | Line: ## 🌟 Recursive Application | Comment: ## 🌟 Recursive Application",README.md,README.md,🌟 Recursive Application,heading,206,2025-03-28
"Previous lines: ``` | ## 🌲 The Cognitive Forest: A Mental Model | CodeSeed approaches code not as isolated files and functions, but as a living ecosystem where meaning emerges from relationships and patterns. Like a forest ecologist studying interconnected life systems, CodeSeed maps your codebase as a rich, multi-layered environment | Line: - **Trees** (Files): Individual units with unique growth patterns and characteristics",README.md,README.md,**Trees** (Files): Individual units with unique growth patterns and characteristics,list_item,19,2025-03-28
"Previous lines: ## 🌲 The Cognitive Forest: A Mental Model | CodeSeed approaches code not as isolated files and functions, but as a living ecosystem where meaning emerges from relationships and patterns. Like a forest ecologist studying interconnected life systems, CodeSeed maps your codebase as a rich, multi-layered environment: | - **Trees** (Files): Individual units with unique growth patterns and characteristic | Line: - **Roots** (Dependencies): Hidden connections that nourish and stabilize the ecosystem",README.md,README.md,**Roots** (Dependencies): Hidden connections that nourish and stabilize the ecosystem  ,list_item,21,2025-03-28
"Previous lines: CodeSeed approaches code not as isolated files and functions, but as a living ecosystem where meaning emerges from relationships and patterns. Like a forest ecologist studying interconnected life systems, CodeSeed maps your codebase as a rich, multi-layered environment: | - **Trees** (Files): Individual units with unique growth patterns and characteristic | - **Roots** (Dependencies): Hidden connections that nourish and stabilize the ecosystem | Line: - **Canopy** (Interfaces): Where the system interfaces with external environments",README.md,README.md,**Canopy** (Interfaces): Where the system interfaces with external environments,list_item,22,2025-03-28
Previous lines: - **Trees** (Files): Individual units with unique growth patterns and characteristic | - **Roots** (Dependencies): Hidden connections that nourish and stabilize the ecosystem | - **Canopy** (Interfaces): Where the system interfaces with external environment | Line: - **Forest Floor** (Shared Resources): Common utilities and resources that support everything,README.md,README.md,**Forest Floor** (Shared Resources): Common utilities and resources that support everything,list_item,23,2025-03-28
Previous lines: - **Roots** (Dependencies): Hidden connections that nourish and stabilize the ecosystem | - **Canopy** (Interfaces): Where the system interfaces with external environment | - **Forest Floor** (Shared Resources): Common utilities and resources that support everythin | Line: - **Seeds** (Patterns): Reusable concepts that can grow into new structures,README.md,README.md,**Seeds** (Patterns): Reusable concepts that can grow into new structures,list_item,24,2025-03-28
Previous lines: - **Canopy** (Interfaces): Where the system interfaces with external environment | - **Forest Floor** (Shared Resources): Common utilities and resources that support everythin | - **Seeds** (Patterns): Reusable concepts that can grow into new structure | Line: - **Growth Rings** (Documentation): Historical records that tell the story of evolution,README.md,README.md,**Growth Rings** (Documentation): Historical records that tell the story of evolution,list_item,25,2025-03-28
Previous lines: ## 🧭 Navigational Guide: Maps for Exploration | ```javascrip | /* | Line: * CodeSeed orientation schema - defines the primary cognitive lenses,README.md,README.md,CodeSeed orientation schema - defines the primary cognitive lenses,list_item,33,2025-03-28
Previous lines: ```javascrip | /* | * CodeSeed orientation schema - defines the primary cognitive lense | Line: * through which code forests can be understood and navigated.,README.md,README.md,through which code forests can be understood and navigated.,list_item,34,2025-03-28
"Previous lines: CodeSeed transforms abstract code structures into intuitive forest metaphors. Instead of seeing disconnected files and functions, you'll see a living ecosystem where each element plays a recognizable role—making complex architectures immediately more approachable. | ### Adaptive Interfaces | The three CSV maps offer multiple entry points for different cognitive styles | Line: - Visual thinkers might start with relationship patterns",README.md,README.md,Visual thinkers might start with relationship patterns,list_item,123,2025-03-28
Previous lines: ### Adaptive Interfaces | The three CSV maps offer multiple entry points for different cognitive styles | - Visual thinkers might start with relationship pattern | Line: - Detail-oriented minds might begin with identifier catalogs,README.md,README.md,Detail-oriented minds might begin with identifier catalogs,list_item,124,2025-03-28
Previous lines: The three CSV maps offer multiple entry points for different cognitive styles | - Visual thinkers might start with relationship pattern | - Detail-oriented minds might begin with identifier catalog | Line: - Big-picture thinkers might explore documentation first,README.md,README.md,Big-picture thinkers might explore documentation first,list_item,125,2025-03-28
"Previous lines: By making code relationships explicit and navigable, CodeSeed transforms passive reading into active exploration. Users naturally discover patterns and connections as they move through the forest, building a mental model that grows increasingly sophisticated. | ## 🌐 META4 Alignment | CodeSeed's implementation follows the META4 framework at every level | Line: - **M**etadata: Making code structure explicit through comprehensive documentation",README.md,README.md,**M**etadata: Making code structure explicit through comprehensive documentation,list_item,138,2025-03-28
Previous lines: ## 🌐 META4 Alignment | CodeSeed's implementation follows the META4 framework at every level: | - **M**etadata: Making code structure explicit through comprehensive documentatio | Line: - **E**xtraction: Identifying patterns across files and directories,README.md,README.md,**E**xtraction: Identifying patterns across files and directories,list_item,140,2025-03-28
Previous lines: CodeSeed's implementation follows the META4 framework at every level: | - **M**etadata: Making code structure explicit through comprehensive documentatio | - **E**xtraction: Identifying patterns across files and directorie | Line: - **T**emplate: Creating structured representations of code elements,README.md,README.md,**T**emplate: Creating structured representations of code elements,list_item,141,2025-03-28
Previous lines: - **M**etadata: Making code structure explicit through comprehensive documentatio | - **E**xtraction: Identifying patterns across files and directorie | - **T**emplate: Creating structured representations of code element | Line: - **A**ssembly: Combining individual analyses into coherent system understanding,README.md,README.md,**A**ssembly: Combining individual analyses into coherent system understanding,list_item,142,2025-03-28
Previous lines: - **E**xtraction: Identifying patterns across files and directorie | - **T**emplate: Creating structured representations of code element | - **A**ssembly: Combining individual analyses into coherent system understandin | Line: - **4** (Metaphor): Bridging technical code structure with intuitive forest metaphors,README.md,README.md,**4** (Metaphor): Bridging technical code structure with intuitive forest metaphors,list_item,143,2025-03-28
"Previous lines: ## 🔍 Semantic Bridge Protocol | When exploring your code forest through DataTrellis, use these questions to navigate between different maps: | 1. **Structure ↔ Semantics**: ""How does the directory organization reflect conceptual organization? | Line: - Compare directory structure in `files.csv` with naming patterns in `identifiers.csv`",README.md,README.md,Compare directory structure in `files.csv` with naming patterns in `identifiers.csv`,list_item,150,2025-03-28
"Previous lines: 1. **Structure ↔ Semantics**: ""How does the directory organization reflect conceptual organization? | - Compare directory structure in `files.csv` with naming patterns in `identifiers.csv` | 2. **Documentation ↔ Implementation**: ""How do explanations align with actual code? | Line: - Cross-reference documentation density in `files.csv` with details in `documentation.csv`",README.md,README.md,Cross-reference documentation density in `files.csv` with details in `documentation.csv`,list_item,153,2025-03-28
"Previous lines: 2. **Documentation ↔ Implementation**: ""How do explanations align with actual code? | - Cross-reference documentation density in `files.csv` with details in `documentation.csv` | 3. **Temporal ↔ Cognitive**: ""How do development patterns reflect thought processes? | Line: - Look for relationships between modification dates and cognitive markers",README.md,README.md,Look for relationships between modification dates and cognitive markers,list_item,156,2025-03-28
"Previous lines: 3. **Temporal ↔ Cognitive**: ""How do development patterns reflect thought processes? | - Look for relationships between modification dates and cognitive markers | 4. **Patterns ↔ Instances**: ""How do architectural choices manifest in specific files? | Line: - Connect pattern counts in `files.csv` with specific implementations",README.md,README.md,Connect pattern counts in `files.csv` with specific implementations,list_item,159,2025-03-28
"Previous lines: For those using this document as a Context Initialization Template, these anchors provide stable reference points: | ```javascrip | /* | Line: * Cognitive Framework: ForestFirst",README.md,README.md,Cognitive Framework: ForestFirst,list_item,177,2025-03-28
"Previous lines: ```javascrip | /* | * Cognitive Framework: ForestFirs | Line: * 
 * Primary Lens: Ecological relationships over isolated components",README.md,README.md,* Primary Lens: Ecological relationships over isolated components,list_item,178,2025-03-28
Previous lines: * Cognitive Framework: ForestFirs | * | * Primary Lens: Ecological relationships over isolated component | Line: * Information Layer: Metadata-rich environment with explicit connections,README.md,README.md,Information Layer: Metadata-rich environment with explicit connections,list_item,180,2025-03-28
Previous lines: * | * Primary Lens: Ecological relationships over isolated component | * Information Layer: Metadata-rich environment with explicit connection | Line: * Navigation Style: Explore from canopy to roots based on curiosity,README.md,README.md,Navigation Style: Explore from canopy to roots based on curiosity,list_item,181,2025-03-28
Previous lines: * Primary Lens: Ecological relationships over isolated component | * Information Layer: Metadata-rich environment with explicit connection | * Navigation Style: Explore from canopy to roots based on curiosit | Line: * Core Invariant: Code as living ecosystem not static artifact,README.md,README.md,Core Invariant: Code as living ecosystem not static artifact,list_item,182,2025-03-28
Previous lines: * Core Invariant: Code as living ecosystem not static artifac | */ | /* | Line: * Communication Protocol: TreeRings,README.md,README.md,Communication Protocol: TreeRings,list_item,186,2025-03-28
"Previous lines: */ | /* | * Communication Protocol: TreeRing | Line: * 
 * Structural Layer: Physical organization of code components",README.md,README.md,* Structural Layer: Physical organization of code components,list_item,187,2025-03-28
Previous lines: * Communication Protocol: TreeRing | * | * Structural Layer: Physical organization of code component | Line: * Semantic Layer: Meaning and intention behind implementations,README.md,README.md,Semantic Layer: Meaning and intention behind implementations,list_item,189,2025-03-28
Previous lines: * | * Structural Layer: Physical organization of code component | * Semantic Layer: Meaning and intention behind implementation | Line: * Temporal Layer: Evolution and development patterns over time,README.md,README.md,Temporal Layer: Evolution and development patterns over time,list_item,190,2025-03-28
Previous lines: * Structural Layer: Physical organization of code component | * Semantic Layer: Meaning and intention behind implementation | * Temporal Layer: Evolution and development patterns over tim | Line: * Cognitive Layer: Thought processes and mental models,README.md,README.md,Cognitive Layer: Thought processes and mental models,list_item,191,2025-03-28
Previous lines: * Cognitive Layer: Thought processes and mental model | */ | /* | Line: * Collaboration Bridge: SeedSharing,README.md,README.md,Collaboration Bridge: SeedSharing,list_item,195,2025-03-28
"Previous lines: */ | /* | * Collaboration Bridge: SeedSharin | Line: * 
 * Entry Point: Generated CSVs as forest maps",README.md,README.md,* Entry Point: Generated CSVs as forest maps,list_item,196,2025-03-28
Previous lines: * Collaboration Bridge: SeedSharin | * | * Entry Point: Generated CSVs as forest map | Line: * Bridge Mechanism: Shared metaphorical framework,README.md,README.md,Bridge Mechanism: Shared metaphorical framework,list_item,198,2025-03-28
Previous lines: * | * Entry Point: Generated CSVs as forest map | * Bridge Mechanism: Shared metaphorical framewor | Line: * Translation Layer: Technical concepts ↔ Forest analogies,README.md,README.md,Translation Layer: Technical concepts ↔ Forest analogies,list_item,199,2025-03-28
Previous lines: * Entry Point: Generated CSVs as forest map | * Bridge Mechanism: Shared metaphorical framewor | * Translation Layer: Technical concepts ↔ Forest analogie | Line: * Expansion Protocol: New lenses as forest perspectives,README.md,README.md,Expansion Protocol: New lenses as forest perspectives,list_item,200,2025-03-28
Previous lines: # 🌱 CodeSeed: Semantic Forest Mapper | > **A Context Initialization Template and Documentation Guide** | Line: ```css,README.md,README.md,css,code_block,5,2025-03-28
"Previous lines: --intention-visibility: ""transparent-by-design"" | --accessibility-level: ""progressive-revelation"" | --relationship-emphasis: ""connections-over-components"" | Line: ```",README.md,README.md,,code_block,14,2025-03-28
"Previous lines: - **Growth Rings** (Documentation): Historical records that tell the story of evolution | This forest metaphor isn't just descriptive—it's a cognitive tool that transforms how you understand code relationships, making complex systems more approachable through familiar natural patterns. | ## 🧭 Navigational Guide: Maps for Exploration | Line: ```javascript",README.md,README.md,javascript,code_block,31,2025-03-28
"Previous lines: entryPoint: ""Comments, TODOs, and implementation choices"" | questions: [""What was the developer thinking?"", ""What mental models guided decisions?"" | } | Line: ```",README.md,README.md,,code_block,62,2025-03-28
"Previous lines: Each map is designed as a rich CSV dataset ready for exploration in DataTrellis, allowing you to filter, sort, and visualize your codebase in ways that reveal its hidden structures and intentions. | ## 🛠️ Practical Usage Guide | ### Installation | Line: ```bash",README.md,README.md,bash,code_block,78,2025-03-28
Previous lines: cd codeseed | # Make the script executabl | chmod +x codeseed.p | Line: ```,README.md,README.md,,code_block,87,2025-03-28
Previous lines: chmod +x codeseed.p | ``` | ### Basic Usage | Line: ```bash,README.md,README.md,bash,code_block,91,2025-03-28
"Previous lines: ./codeseed.py --output-dir ./analysis | # Exclude pattern | ./codeseed.py --exclude ""node_modules"" --exclude "".git | Line: ```",README.md,README.md,,code_block,103,2025-03-28
"Previous lines: --- | ## 📊 Contextual Understanding Anchors | For those using this document as a Context Initialization Template, these anchors provide stable reference points: | Line: ```javascript",README.md,README.md,javascript,code_block,175,2025-03-28
Previous lines: * Translation Layer: Technical concepts ↔ Forest analogie | * Expansion Protocol: New lenses as forest perspective | * | Line: ```,README.md,README.md,,code_block,202,2025-03-28
Previous lines: # 🌱 CodeSeed: Semantic Forest Mapper | Line: > **A Context Initialization Template and Documentation Guide**,README.md,README.md,A Context Initialization Template and Documentation Guide,emphasis,3,2025-03-28
"Previous lines: ``` | ## 🌲 The Cognitive Forest: A Mental Model | CodeSeed approaches code not as isolated files and functions, but as a living ecosystem where meaning emerges from relationships and patterns. Like a forest ecologist studying interconnected life systems, CodeSeed maps your codebase as a rich, multi-layered environment: | Line: - **Trees** (Files): Individual units with unique growth patterns and characteristics",README.md,README.md,Trees,emphasis,20,2025-03-28
"Previous lines: ## 🌲 The Cognitive Forest: A Mental Model | CodeSeed approaches code not as isolated files and functions, but as a living ecosystem where meaning emerges from relationships and patterns. Like a forest ecologist studying interconnected life systems, CodeSeed maps your codebase as a rich, multi-layered environment: | - **Trees** (Files): Individual units with unique growth patterns and characteristic | Line: - **Roots** (Dependencies): Hidden connections that nourish and stabilize the ecosystem",README.md,README.md,Roots,emphasis,21,2025-03-28
"Previous lines: CodeSeed approaches code not as isolated files and functions, but as a living ecosystem where meaning emerges from relationships and patterns. Like a forest ecologist studying interconnected life systems, CodeSeed maps your codebase as a rich, multi-layered environment: | - **Trees** (Files): Individual units with unique growth patterns and characteristic | - **Roots** (Dependencies): Hidden connections that nourish and stabilize the ecosystem | Line: - **Canopy** (Interfaces): Where the system interfaces with external environments",README.md,README.md,Canopy,emphasis,22,2025-03-28
Previous lines: - **Trees** (Files): Individual units with unique growth patterns and characteristic | - **Roots** (Dependencies): Hidden connections that nourish and stabilize the ecosystem | - **Canopy** (Interfaces): Where the system interfaces with external environment | Line: - **Forest Floor** (Shared Resources): Common utilities and resources that support everything,README.md,README.md,Forest Floor,emphasis,23,2025-03-28
Previous lines: - **Roots** (Dependencies): Hidden connections that nourish and stabilize the ecosystem | - **Canopy** (Interfaces): Where the system interfaces with external environment | - **Forest Floor** (Shared Resources): Common utilities and resources that support everythin | Line: - **Seeds** (Patterns): Reusable concepts that can grow into new structures,README.md,README.md,Seeds,emphasis,24,2025-03-28
Previous lines: - **Canopy** (Interfaces): Where the system interfaces with external environment | - **Forest Floor** (Shared Resources): Common utilities and resources that support everythin | - **Seeds** (Patterns): Reusable concepts that can grow into new structure | Line: - **Growth Rings** (Documentation): Historical records that tell the story of evolution,README.md,README.md,Growth Rings,emphasis,25,2025-03-28
"Previous lines: } | ``` | CodeSeed generates three primary maps to help you navigate your code forest: | Line: 1. **🌱 Forest Census (`files.csv`)**: A comprehensive inventory of all ""trees"" with their vital statistics, health indicators, and growth patterns",README.md,README.md,🌱 Forest Census (`files.csv`),emphasis,66,2025-03-28
"Previous lines: ``` | CodeSeed generates three primary maps to help you navigate your code forest: | 1. **🌱 Forest Census (`files.csv`)**: A comprehensive inventory of all ""trees"" with their vital statistics, health indicators, and growth patterns | Line: 2. **🍃 Leaf Patterns (`identifiers.csv`)**: A detailed catalog of all identifiers—the leaves through which your code breathes—with their contexts, relationships, and semantic meanings",README.md,README.md,🍃 Leaf Patterns (`identifiers.csv`),emphasis,68,2025-03-28
"Previous lines: CodeSeed generates three primary maps to help you navigate your code forest: | 1. **🌱 Forest Census (`files.csv`)**: A comprehensive inventory of all ""trees"" with their vital statistics, health indicators, and growth patterns | 2. **🍃 Leaf Patterns (`identifiers.csv`)**: A detailed catalog of all identifiers—the leaves through which your code breathes—with their contexts, relationships, and semantic meanings | Line: 3. **📜 Forest Chronicles (`documentation.csv`)**: An anthology of all documentation—the stories your code tells about itself—revealing intentions, explanations, and cognitive markers",README.md,README.md,📜 Forest Chronicles (`documentation.csv`),emphasis,70,2025-03-28
"Previous lines: By making code relationships explicit and navigable, CodeSeed transforms passive reading into active exploration. Users naturally discover patterns and connections as they move through the forest, building a mental model that grows increasingly sophisticated. | ## 🌐 META4 Alignment | CodeSeed's implementation follows the META4 framework at every level: | Line: - **M**etadata: Making code structure explicit through comprehensive documentation",README.md,README.md,M,emphasis,139,2025-03-28
Previous lines: ## 🌐 META4 Alignment | CodeSeed's implementation follows the META4 framework at every level: | - **M**etadata: Making code structure explicit through comprehensive documentatio | Line: - **E**xtraction: Identifying patterns across files and directories,README.md,README.md,E,emphasis,140,2025-03-28
Previous lines: CodeSeed's implementation follows the META4 framework at every level: | - **M**etadata: Making code structure explicit through comprehensive documentatio | - **E**xtraction: Identifying patterns across files and directorie | Line: - **T**emplate: Creating structured representations of code elements,README.md,README.md,T,emphasis,141,2025-03-28
Previous lines: - **M**etadata: Making code structure explicit through comprehensive documentatio | - **E**xtraction: Identifying patterns across files and directorie | - **T**emplate: Creating structured representations of code element | Line: - **A**ssembly: Combining individual analyses into coherent system understanding,README.md,README.md,A,emphasis,142,2025-03-28
Previous lines: - **E**xtraction: Identifying patterns across files and directorie | - **T**emplate: Creating structured representations of code element | - **A**ssembly: Combining individual analyses into coherent system understandin | Line: - **4** (Metaphor): Bridging technical code structure with intuitive forest metaphors,README.md,README.md,4,emphasis,143,2025-03-28
"Previous lines: - **4** (Metaphor): Bridging technical code structure with intuitive forest metaphors | ## 🔍 Semantic Bridge Protocol | When exploring your code forest through DataTrellis, use these questions to navigate between different maps: | Line: 1. **Structure ↔ Semantics**: ""How does the directory organization reflect conceptual organization?""",README.md,README.md,Structure ↔ Semantics,emphasis,149,2025-03-28
"Previous lines: When exploring your code forest through DataTrellis, use these questions to navigate between different maps: | 1. **Structure ↔ Semantics**: ""How does the directory organization reflect conceptual organization? | - Compare directory structure in `files.csv` with naming patterns in `identifiers.csv` | Line: 2. **Documentation ↔ Implementation**: ""How do explanations align with actual code?""",README.md,README.md,Documentation ↔ Implementation,emphasis,152,2025-03-28
"Previous lines: - Compare directory structure in `files.csv` with naming patterns in `identifiers.csv` | 2. **Documentation ↔ Implementation**: ""How do explanations align with actual code? | - Cross-reference documentation density in `files.csv` with details in `documentation.csv` | Line: 3. **Temporal ↔ Cognitive**: ""How do development patterns reflect thought processes?""",README.md,README.md,Temporal ↔ Cognitive,emphasis,155,2025-03-28
"Previous lines: - Cross-reference documentation density in `files.csv` with details in `documentation.csv` | 3. **Temporal ↔ Cognitive**: ""How do development patterns reflect thought processes? | - Look for relationships between modification dates and cognitive markers | Line: 4. **Patterns ↔ Instances**: ""How do architectural choices manifest in specific files?""",README.md,README.md,Patterns ↔ Instances,emphasis,158,2025-03-28
